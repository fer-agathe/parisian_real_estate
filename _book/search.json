[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Geospatial Disparities: A Case Study on Real Estate Prices in Paris",
    "section": "",
    "text": "Introduction\nThis ebook provides the replication codes to the article titled ‘Geospatial Disparities: A Case Study on Real Estate Prices in Paris.’\nThe ebook is divided in 5 chapters. In Chapter 1, we present the datasets used in the article along with some descriptive statistics. In Chapter 2, we show are we can proceed to spatially smooth geospatial data. In Chapter 3, we present how to visualize the calibration and how to compute the expected calibration error. In Chapter 4, we show spatial disparities in terms of fairness. Lastly, in Chapter 5, we present strategies to mitigate the biases highlighted in the previous chapters."
  },
  {
    "objectID": "index.html#abstract-of-the-article",
    "href": "index.html#abstract-of-the-article",
    "title": "Geospatial Disparities: A Case Study on Real Estate Prices in Paris",
    "section": "Abstract of the Article",
    "text": "Abstract of the Article\nDriven by an increasing prevalence of trackers, ever more IoT sensors, and the declining cost of computing power, geospatial information has come to play a pivotal role in contemporary predictive models. While enhancing prognostic performance, geospatial data also has the potential to perpetuate many historical socio-economic patterns, raising concerns about a resurgence of biases and exclusionary practices and their disproportionate impacts on society. Addressing this, our paper emphasizes the crucial need to identify and rectify such biases and calibration errors in predictive models, particularly as algorithms become more intricate and less interpretable. The increasing granularity of geospatial information further introduces ethical concerns, as choosing different geographical scales may exacerbate disparities akin to redlining and exclusionary zoning. To address these issues, we propose a toolkit for identifying and mitigating biases arising from geospatial data. Extending classical fairness definitions, we incorporate an ordinal regression case with spatial attributes, deviating from the binary classification focus. This extension allows us to gauge disparities stemming from data aggregation levels and advocates for a less interfering correction approach. Illustrating our methodology using a Parisian real estate dataset, we showcase practical applications and scrutinize the implications of choosing geographical aggregation levels for fairness and calibration measures.\nKeywords: Artificial intelligence, Machine learning, Geospatial Data, Fairness, Calibration\n\n\n\n\n\n\nDisclaimer\n\n\n\nWe were kindly provided with estimate price and selling price on the Parisian real-estate market.\nThe data utilized in this study are sourced from Meilleurs Agents, a French Real Estate platform that produces data on the residential market and operates a free online automatic valuation model (AVM).\nIt is imperative to underscore that the primary objective of this paper is not to scrutinize the inherent biases or calibration of the predictive model employed by the company. Based on our analysis, the company’s model appears to be well calibrated.\nOur focus is on investigating biases that may arise from geospatial data and the potential socio-economic implications of these biases. We specifically introduce sensitive attributes related to geographical regions as part of our fairness analysis. This illustrative case involving a Parisian real estate dataset serves as a practical example to demonstrate our methodology."
  },
  {
    "objectID": "data.html#source",
    "href": "data.html#source",
    "title": "1  Data",
    "section": "1.1 Source",
    "text": "1.1 Source\nThe data come from Meilleurs Agents, a French Real Estate platform that produces data on the residential market and operates a free online automatic valuation model (AVM).\n\nload(\"../data/raw/base_immo.RData\")"
  },
  {
    "objectID": "data.html#sec-data-stat-desc",
    "href": "data.html#sec-data-stat-desc",
    "title": "1  Data",
    "section": "1.2 Global Summary Statistics",
    "text": "1.2 Global Summary Statistics\nWe have access to both the estimated price \\(\\hat{Z}\\) of the underlying property and realized net sale price \\(Z\\). We also have access to the approximate location and amount of square meters (\\(m^2\\)) of the property.\nWe restrict our observations to cases where all the information is available.\n\ndata_immo_all &lt;- \n  base_finale |&gt; \n  as_tibble() |&gt; \n  mutate(CODE_IRIS = as.character(iris_cog)) |&gt; \n  mutate(difference_total = contract_final_net_price - backtest_estima) |&gt; \n  mutate(pm2_estimated = backtest_estima/arrea_carrez) |&gt; \n  mutate(difference_pm2 = pm2 - pm2_estimated) |&gt; \n  select(-c(X))\nnrow(data_immo_all)\n\n[1] 25675\n\n\nThere are 25,675 observation in the dataset. Let us have a look at the number of observations depending on the city. The dataset encompasses data from Paris intra-muros and from other cities within the French departement `Île-de-France’.\n\ndata_immo_all &lt;- data_immo_all |&gt; mutate(\n  location_city = ifelse(\n    city_name == \"Paris\", \n    yes = \"Paris - intra-muros\", no = \"Paris - Île-de-France\")\n)\n\ndata_immo_all |&gt; count(location_city)\n\n# A tibble: 2 × 2\n  location_city             n\n  &lt;chr&gt;                 &lt;int&gt;\n1 Paris - intra-muros   11906\n2 Paris - Île-de-France 13769\n\n\nLet us visualize the distribution of prices per square meter by location.\n\n\nDisplay the codes used to create the Figure.\nggplot(\n  data = data_immo_all,\n  mapping = aes(x = pm2, fill = location_city)) +\n  geom_histogram(position = \"dodge\") +\n  scale_fill_manual(\n    \"location\",\n    values = c(\n      \"Paris - Île-de-France\" = colors_[3], \n      \"Paris - intra-muros\" = colors_[2])\n    ) +\n  scale_x_continuous(\n    labels = scales::label_comma(), \n    limits = c(0, 20000)\n  ) +\n  labs(x = latex2exp::TeX(\"Price per $m^2$\"), y = \"Observations\") +\n  global_theme() +\n  theme(legend.position = \"bottom\")\n\n\n\n\nFigure 1.1: Price per square meter in different areas of our data. Paris intra-muros refers to the 20 arrondissements that constitute the core of the city, Ile de France refers to the remaining metropolitan area.\n\n\n\n\n\nWe restrict ourselves to Paris intra-muros.\n\ndata_immo &lt;- data_immo_all |&gt; \n  filter(city_name == \"Paris\")\nnrow(data_immo)\n\n[1] 11906\n\n\nThis leaves us with 11,906 cases. The range of the sale agreement date is:\n\nrange(data_immo$sale_agreement_date)\n\n[1] \"2019-01-01\" \"2019-12-31\"\n\n\nWe use the prices per \\(m^2\\) to normalize the errors by property size.\n\n\nDisplay the codes used to create the Figure.\nggplot(\n  data = data_immo |&gt; select(pm2, pm2_estimated) |&gt; \n    pivot_longer(cols = c(pm2, pm2_estimated)) |&gt; \n    mutate(\n      name = factor(\n        name, \n        levels = c(\"pm2\", \"pm2_estimated\"), \n        labels = c(\"Observed price\", \"Estimated price\")\n      )\n    ),\n  mapping = aes(x = value)\n) +\n  geom_histogram(position = \"identity\", colour = \"white\") +\n  facet_wrap(~name) +\n  labs(\n    x = latex2exp::TeX(\"Price per $m^2$ (thousand Euros)\"),\n    y = \"Number of observations\"\n  ) +\n  scale_x_continuous(\n    label = scales::label_comma(scale = 1/1000), \n    limits = c(0, 20000)\n  ) +\n  scale_y_continuous(label = scales::label_comma()) +\n  global_theme()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 124 rows containing non-finite values (`stat_bin()`).\n\n\nWarning: Removed 4 rows containing missing values (`geom_bar()`).\n\n\n\n\nFigure 1.2: Distribution of observed and estimated prices.\n\n\n\n\n\nLet us also create a table with basic summary statistics for the price variables. First, we add labels to the columns of interest here.\n\ndata_immo &lt;- data_immo |&gt; \n  labelled::set_variable_labels(\n    pm2 = \"Observed Price per square meter\",\n    pm2_estimated = \"Estimated Price per square meter\"\n  )\n\nTable 1.1 shows standard statistics for both the observed and the estimated price.\n\n\nDisplay the codes to create the Table.\ndata_immo |&gt; \n  tbl_summary(\n    include = c(\"pm2\", \"pm2_estimated\"),\n    type = all_continuous() ~ \"continuous2\",\n    statistic = list(\n      all_continuous() ~ c(\"{mean} ({sd})\", \"{median} ({p25}, {p75})\"),\n      all_categorical() ~ \"{n} ({p}%)\"),\n    digits = list(\n      all_continuous() ~ 2,\n      all_categorical() ~ 0\n    )\n  ) |&gt; \n  modify_header(label ~ \"**Variable**\") |&gt;\n  add_stat_label(\n    label = list(\n      all_continuous() ~ c(\"Mean (std)\", \"Median (Q1, Q3)\"),\n      all_categorical() ~ \"n (%)\"\n    )\n  )\n\n\n\n\n\n\nTable 1.1:  Descriptive statistics on prices. \n  \n    \n    \n      Variable\n      N = 11,906\n    \n  \n  \n    Observed Price per square meter\n\n        Mean (std)\n10,784.21 (3,563.00)\n        Median (Q1, Q3)\n10,387.07 (9,200.00, 11,852.03)\n    Estimated Price per square meter\n\n        Mean (std)\n10,537.50 (2,408.10)\n        Median (Q1, Q3)\n10,255.28 (9,172.82, 11,506.41)\n  \n  \n  \n\n\n\n\n\nLet us remove outliers with a price per square meter of over €20,000 and observations from mostly commercial areas.\n\ndata_immo &lt;- \n  data_immo |&gt; \n  filter(pm2 &lt;= 20000)\n\nIn all, we then have access to 11,812 observations after these basic cleaning steps."
  },
  {
    "objectID": "data.html#sec-data-iris",
    "href": "data.html#sec-data-iris",
    "title": "1  Data",
    "section": "1.3 IRIS",
    "text": "1.3 IRIS\nOur Data contains geospatial information, aggregated at the IRIS (Ilots Regroupés pour l’Information Statistique) level, a statistical unit defined and published by the French National Institute of Statistics and Economic Studies.1 The city of Paris is divided into 20 arrondissements. Each IRIS located in Paris belongs to a single arrondissement.\n\n\n\n\n\n\nTip\n\n\n\nThree types of IRIS are distinguished:\n\nResidential IRIS: their population generally ranges between 1,800 and 5,000 inhabitants. They are homogeneous in terms of housing type, and their boundaries are based on major breaks in the urban fabric (main roads, railways, watercourses, …).\nThe IRIS for economic activity: they bring together more than 1,000 employees and have at least twice as many salaried jobs as resident population;\nMiscellaneous IRIS: these are large, specific, sparsely populated areas with significant surface areas (amusement parks, port areas, forests, …).\n\nSource: Limites IRIS - Descriptif de contenu\n\n\nThe total number of IRIS is:\n\nlength(unique(data_immo$iris_name))\n\n[1] 880\n\n\nTo plot maps, we first need to get a Parisian map. We use the `Contours… IRIS®’ shapefile from géoservices (édition 2023).\n\nshapes_iris &lt;- str_c(\n  \"../data/geo/CONTOURS-IRIS_3-0__SHP__FRA_2023-01-01/CONTOURS-IRIS/\",\n  \"1_DONNEES_LIVRAISON_2023-09-00134/CONTOURS-IRIS_3-0_SHP_LAMB93_FXX-2023\"\n  ) |&gt; \n  sf::st_read(layer=\"CONTOURS-IRIS\", quiet = TRUE)\n\nWe extract the Seine River for better looking maps:\n\nshapes_seine &lt;- \n  shapes_iris |&gt; \n  filter(grepl('Paris ', NOM_COM)) |&gt; \n  filter(grepl('Seine ', NOM_IRIS))\n\nTo display the Seine River with a single contour, let us create an union of all the polygons that define it:\n\nshapes_seine &lt;- \n  shapes_seine |&gt; \n  summarise(\n    geo_ = st_union(geometry)\n  )\n\nLet us focus only on Paris intra-muros:\n\nshapes_paris &lt;- \n  shapes_iris |&gt; \n  filter(grepl('Paris ', NOM_COM)) |&gt; \n  mutate(size_poly = as.numeric(st_area(geometry))) |&gt; \n  filter(size_poly &lt; 1840733.0)\n\nLet us save these maps objects for later use.\n\nsave(shapes_paris, shapes_seine, file = \"../data/shapes.rda\")\n\nLet us also export a table with all the IRIS codes in each arrondissement.\n\naggregation_arrond &lt;- \n  shapes_paris |&gt; \n  select(NOM_COM, CODE_IRIS) |&gt; \n  st_drop_geometry() |&gt; \n  as_tibble() |&gt; \n  write_csv('../data/geo/aggregation_arrond.csv')"
  },
  {
    "objectID": "data.html#sec-data-wealth",
    "href": "data.html#sec-data-wealth",
    "title": "1  Data",
    "section": "1.4 Wealth Level per IRIS",
    "text": "1.4 Wealth Level per IRIS\nThe welth level per IRIS comes from the `Revenus, pauvreté et niveau de vie en 2020 (Iris)’ distributed by the National Institute of Statistics and Economic Studies (INSEE). The data can be downloaded free of charge at the following addreess: https://www.insee.fr/fr/statistiques/7233950#consulter.\n\ndata_income &lt;- read_delim(\n  str_c(\n    \"../data/econ/\", \n    \"BASE_TD_FILO_DISP_IRIS_2020_CSV/BASE_TD_FILO_DISP_IRIS_2020.csv\"\n  ),\n  delim = \";\",\n  escape_double = FALSE,\n  trim_ws = TRUE\n)\n\nThe median income per IRIS can easily be computed.\n\nmedian_inc_data &lt;- \n  data_income |&gt; \n  select(CODE_IRIS = IRIS, DISP_MED20) |&gt; \n  mutate(DISP_MED20 = as.numeric(DISP_MED20))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `DISP_MED20 = as.numeric(DISP_MED20)`.\nCaused by warning:\n! NAs introduced by coercion\n\nhead(median_inc_data)\n\n# A tibble: 6 × 2\n  CODE_IRIS DISP_MED20\n  &lt;chr&gt;          &lt;dbl&gt;\n1 010040101      19450\n2 010040102      17910\n3 010040201      20210\n4 010040202      24870\n5 010330102      19810\n6 010330103      23770\n\n\n\n\n\n\n\n\nNote\n\n\n\nA choropleth map with smoothed values is shown in Section 2.6 in Chapter 2.\n\n\nFrom the Parisian map, we extract some information at the IRIS level: the name of the arrondissement (NOM_COM), the name of the IRIS (NOM_IRIS), and the type of IRIS (TYP_IRIS).\n\niris_supplementary_data &lt;- \n  shapes_paris |&gt; \n  as_tibble() |&gt; \n  select(CODE_IRIS, NOM_COM, NOM_IRIS, TYP_IRIS)\n\nThis information can be added to the real estate data. We will also add income data at the IRIS level in the dataset and define a new categorical variable: income_class which takes three values:\n\n\"rich\" if the income in the IRIS is larger or equal to €35,192,\n\"poor\" if the income in the IRIS is lower or equal to €20,568,\n\"others\" otherwise.\n\n\ndata_immo &lt;- \n  data_immo |&gt; \n  # Reduce to Parisan data\n  inner_join(iris_supplementary_data, by = \"CODE_IRIS\") |&gt; \n  left_join(\n    data_income |&gt; \n      select(CODE_IRIS = IRIS, median_income = DISP_MED20) |&gt; \n      mutate(median_income = as.numeric(median_income)),\n    by = \"CODE_IRIS\"\n  ) |&gt; \n  mutate(\n    income_class = if_else(\n      median_income &gt;= 35192,\n      'rich', \n      if_else(median_income &lt;= 20568, 'poor', 'others')\n    )\n  )\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `median_income = as.numeric(median_income)`.\nCaused by warning:\n! NAs introduced by coercion"
  },
  {
    "objectID": "data.html#sec-export-data",
    "href": "data.html#sec-export-data",
    "title": "1  Data",
    "section": "1.5 Export Data",
    "text": "1.5 Export Data\nWe restrict ourselves to the residential IRIS (type \"H\") and to sales where both the estimated and the observed price per square meter was below 20,000.\n\ndata_clean_all &lt;- \n  data_immo |&gt; \n  filter(TYP_IRIS == 'H')  |&gt; \n  filter(pm2_estimated &lt; 20000) |&gt; \n  filter(pm2 &lt; 20000) \n\nThen, we save the table for later use.\n\nsave(data_clean_all, file = \"../data/data_clean_all.rda\")\nwrite_csv(data_clean_all, file = \"../data/data_clean_all.csv\")"
  },
  {
    "objectID": "data.html#footnotes",
    "href": "data.html#footnotes",
    "title": "1  Data",
    "section": "",
    "text": "For more information on IRIS, refer to INSEE.↩︎"
  },
  {
    "objectID": "neigh-smoothing.html#load-data",
    "href": "neigh-smoothing.html#load-data",
    "title": "2  Neighborhood-Based Smoothing",
    "section": "2.1 Load Data",
    "text": "2.1 Load Data\nLet us load the data obtained in Chapter 1.\n\n# Real Estate\nload(\"../data/data_clean_all.rda\")\n# Maps files\nload(\"../data/shapes.rda\")\n\n\n\nDisplay the codes used to create the Figure\nshapes_paris |&gt; \n  left_join(\n    data_clean_all |&gt; \n      group_by(CODE_IRIS) |&gt; \n      summarise(\n        total_observations = n(), \n        median_price = median(contract_final_net_price)\n      ),\n    by = \"CODE_IRIS\"\n  ) %&gt;% \n  replace_na(list(total_observations = 0)) |&gt; \n  ggplot() + \n  geom_sf(aes(fill = total_observations)) + \n  scale_fill_gradientn(\n    colours = rev(terrain.colors(10)), \n    name = 'No. Observations', \n    position = 'bottom'\n    ) +\n  geom_sf(\n    data = shapes_seine, \n    fill = col_seine\n  ) + \n  global_theme()\n\n\n\n\nFigure 2.1: Number of observation per IRIS."
  },
  {
    "objectID": "neigh-smoothing.html#defining-neighbors",
    "href": "neigh-smoothing.html#defining-neighbors",
    "title": "2  Neighborhood-Based Smoothing",
    "section": "2.2 Defining Neighbors",
    "text": "2.2 Defining Neighbors\nThe size of the area of each IRIS differs considerably. Hence, smoothing the observation by simply defining higher levels by Euclidean distance from a given point might pose problems, as it could include many dense but heterogeneous regions on one end of the spectrum or only a few on the other. Instead, we will define a neighborhood graph (see the paper for a formal definition).\nWe will hereafter denote \\(i\\) and \\(j\\) the polygons of the ith and jth IRIS, respectively.\nThe neighborhood graph can be represented by an \\(n\\times n\\) matrix named an adjacency matrix, denoted \\(A\\).If regions \\(i\\) and \\(j\\) intersect, \\(A_{i,j}=1\\), and \\(A_{i,j}=0\\) otherwise. Representing neighborhood relations using an adjacency matrix has the advantage that non-intermediate connections can be easily obtained by successively multiplying the adjacency matrix by itself. That is, all nonzero elements of \\(A^2\\) represent neighborhoods that are either immediately adjacent to a given region (the direct neighbors) or are adjacent to the direct neighbors (the neighbors of the direct neighbors). This process can be repeated \\(n\\) times to obtain neighbors that can be reached within an \\(n\\) length path. This provides a more natural way to define neighborhoods, as with each increasing path length, an entire homogeneous region is added to the higher-level aggregation.\nLet us construct a neighborhood graphs with R, using {igraph}.\n\nlibrary(igraph)\n\nHere we need the Seine River as well. Otherwise, we cannot find neighbors.\n\nshapes_all &lt;- \n  shapes_paris |&gt; \n  bind_rows(shapes_seine)\n\nLet us get the first level:\n\nshapes_neighbours_export &lt;- st_intersects(shapes_all, shapes_all)\n\nThe \\(A_1\\) adjacency matrix can then be constructed. We initialize it with 0s.\n\nadj_matrix_lev_1 &lt;- matrix(\n  nrow = length(shapes_neighbours_export), \n  ncol = length(shapes_neighbours_export), \n  0\n)\n\nThen, we loop over all polygons and change the value of \\(A_1(i,j)\\) to 1 if regions \\(i\\) and \\(j\\) are neighbors:\n\nrow_index &lt;- 1\nfor (item in shapes_neighbours_export) {\n  for (col_idx in item) {\n    adj_matrix_lev_1[row_index, col_idx] = 1\n  }\n  row_index &lt;- row_index + 1\n}\n\nThen, by multiplying this square matrix by itself, we can obtain the neighbors and the neighbors of neighbors:\n\nadj_matrix_lev_2 &lt;- adj_matrix_lev_1 %*% adj_matrix_lev_1\nadj_matrix_lev_2[which(adj_matrix_lev_2 &gt; 0)] &lt;- 1\n\nLet us focus on a particular IRIS and visualize it and its neighbors on a map. We will produce three maps: one which only shows the particular IRIS, another one which also its immediate neighbors, and a third one which shows the neighbors immediately adjacent or adjacent to the direct neighbor.\n\ntarget_community &lt;- \"Enfants Rouges 4\"\n\nThe map with the single target IRIS:\n\nsingle_munip &lt;- \n  shapes_paris %&gt;% \n  rowid_to_column('index_ids') |&gt; \n  filter(grepl('Paris', NOM_COM)) |&gt; \n  mutate(\n    Classification = if_else(\n      NOM_IRIS == target_community,\n      'target community',\n      'others')\n  ) |&gt; \n  mutate(\n    Classification = factor(\n      Classification,\n      levels=c('target community', 'neighbours', 'others')\n    )\n  ) |&gt; \n  ggplot() + \n  geom_sf(aes(fill = Classification)) + \n  scale_fill_manual(\n    values = c(colors_[c(3)], '#D21F3C', colors_[c(1)]), \n    limits = c('target community', 'neighbours', 'others'), \n    name = ''\n  ) + \n  geom_sf(\n    data = shapes_seine, \n    fill = col_seine\n  ) + \n  global_theme() + \n  theme(legend.position = \"bottom\")\n\nThe map with the immediate neighbors:\n\none_neigh &lt;- shapes_paris |&gt; \n  rowid_to_column('index_ids') |&gt; \n  filter(grepl('Paris', NOM_COM)) |&gt; \n  mutate(\n    Classification = if_else(\n      index_ids %in% which(\n        adj_matrix_lev_1[which(shapes_paris$NOM_IRIS == target_community),] == 1\n        ),\n      'neighbours', 'others')\n  ) |&gt; \n  mutate(\n    Classification = if_else(\n      index_ids == which(shapes_paris$NOM_IRIS == target_community), \n      'target community', Classification\n    )\n  ) |&gt; \n  mutate(\n    Classification = factor(\n      Classification,\n      levels=c('target community', 'neighbours', 'others')\n    )\n  ) |&gt; \n  ggplot() + \n  geom_sf(aes(fill = Classification)) + \n  scale_fill_manual(values = c(colors_[c(3)], '#D21F3C', colors_[c(1)])) + \n  geom_sf(\n    data = shapes_seine, \n    fill = col_seine\n  ) + \n  global_theme()\n\nLastly, the third map with the both immediate neighbors and neighbors of neighbors:\n\ntwo_neigh &lt;- \n  shapes_paris |&gt; \n  rowid_to_column('index_ids') |&gt; \n  filter(grepl('Paris', NOM_COM)) |&gt; \n  mutate(\n    Classification = if_else(\n      index_ids %in% which(\n        adj_matrix_lev_2[which(shapes_paris$NOM_IRIS == target_community),] &gt; 0\n      ),\n      'neighbours', \n      'others'\n    )\n  )|&gt; \n  mutate(\n    Classification = if_else(\n      index_ids == which(shapes_paris$NOM_IRIS == target_community), \n      'target community', \n      Classification\n    )\n  ) |&gt; \n  mutate(\n    Classification = factor(\n      Classification,\n      levels=c('target community', 'neighbours', 'others')\n    )\n  ) |&gt; \n  ggplot() + \n  geom_sf(aes(fill = Classification)) + \n  scale_fill_manual(values = c(colors_[c(3)], '#D21F3C', colors_[c(1)])) + \n  geom_sf(\n    data = shapes_seine, \n    fill = col_seine\n  ) + \n  global_theme()\n\nFigure 2.2 illustrates how the neighborhood set for a particular IRIS community can be calculated.\n\n\nDisplay the codes used to create the Figure\np_example_neigh_row &lt;- cowplot::plot_grid(\n  single_munip + theme(legend.position=\"none\"),\n  one_neigh + theme(legend.position=\"none\"),\n  two_neigh + theme(legend.position=\"none\"),\n  align = 'vh',\n  ncol = 3\n)\np_example_neigh_legend &lt;- cowplot::get_legend(single_munip)\n\np_example_neigh &lt;- cowplot::plot_grid(\n  p_example_neigh_row, p_example_neigh_legend, \n  ncol = 1, rel_heights = c(1, .2)\n)\np_example_neigh\n\n\n\n\nFigure 2.2: A sampled IRIS region within Paris (left pane) and its immediate adjacent neighbors (center pane) and the second level neighbors (right pane). The Seine River is depicted in blue whereas all other regions are depicted in yellow."
  },
  {
    "objectID": "neigh-smoothing.html#sec-spatial-price-smoothing",
    "href": "neigh-smoothing.html#sec-spatial-price-smoothing",
    "title": "2  Neighborhood-Based Smoothing",
    "section": "2.3 Spatial Price Smoothing",
    "text": "2.3 Spatial Price Smoothing\nFor a given variable observed at IRIS level \\(x\\), the smoothed value for region \\(r_i\\), denoted as \\(\\lambda_\\omega(x_i)\\) can be written as:\n\\[\\begin{align}\n\\lambda_{\\omega}(x_i) = \\frac{1}{\\sum_{j=1}^{n} {\\omega}(r_i,r_j)}\\sum_{j=1}^n {\\omega}(r_i,r_j) x_i \\enspace.     \n\\end{align}\n\\tag{2.1}\\] For example, let \\(d(r_i, r_j)\\) be the path length between regions \\(r_i\\) and \\(r_j\\), then a simple way to define \\(\\omega(r_i,r_j)\\) is: \\[\n\\begin{align}\\label{eq:weights}\n    \\omega(r_i,r_j)=\n        \\begin{cases}\n        \\frac{1}{(1+d(r_i,r_j))^p},\\quad & \\text{if } d(i,j) \\leq m \\\\\n        0, & \\text{otherwise} \\enspace,\n        \\end{cases}\n\\end{align}\n\\tag{2.2}\\] where \\(p\\) and \\(m\\) are hyperparameters to be selected, similar to the bandwidth operator.\nWe initiate the neighborhood matrix by putting the values we just computed:\n\nneigh_matrix &lt;- adj_matrix_lev_1\n\nThen, we can increase the neighborhood distance \\(m\\) so that it takes values from 2 to 30 in steps of 1. This allows us to get each IRIS which are distant from one another by a value lower or equal to \\(m\\).\n\n# dir.create(\"../data/neighbours/all_neighbour_levels\")\n# cli::cli_progress_bar(total = length(seq(2,30)))\nfor (neigh_idst in seq(2,30)) {\n  # Set neighbors\n  adj_matrix_next_level &lt;- neigh_matrix %*% adj_matrix_lev_1\n  adj_matrix_next_level[which(adj_matrix_next_level &gt; 0)] &lt;- 1\n  \n  # Create Graph \n  graph_tmp &lt;- graph_from_adjacency_matrix(adj_matrix_next_level)\n  export_string &lt;- paste(\n    '../data/neighbours/all_neighbour_levels/neighbours_level_',\n    neigh_idst,\n    '.csv', \n    sep=''\n  )\n  \n  graph_tmp |&gt; \n    as_data_frame() |&gt; \n    mutate(access = neigh_idst) |&gt; \n    write_csv(export_string)\n  \n  # Reset and restart\n  neigh_matrix &lt;- adj_matrix_next_level\n  # cli::cli_progress_update(set = which(neigh_idst == seq(2,30)))\n}\n\nWe also export a tibble with a mapping between each IRIS and its row number in shapes_all.\n\nmapping_neighbours &lt;- \n  shapes_all |&gt; \n  rowid_to_column('index_ids') |&gt; \n  select(index_ids, CODE_IRIS) |&gt; \n  as_tibble() |&gt; \n  select(-c(geometry))\n\nmapping_neighbours |&gt; \n  write_csv('../data/neighbours/mapping_neighbour_id_iris.csv')\n\nNow that we have obtained the adjacency matrices from \\(m=\\{1, 2, \\ldots, 30\\}\\), we can create a tibble which will contain the distance from each polygon to all the other polygons (provided the maximum distance is lower or equal to 30).\nWe first populate the desired object, all_neighbours with all the IRIS which have a distance equal to 1:\n\nall_neighbours &lt;- \n  graph_from_adjacency_matrix(adj_matrix_lev_1) |&gt; \n  as_data_frame() |&gt; \n  tibble() |&gt; \n  mutate(access = 1)\nall_neighbours\n\n# A tibble: 7,559 × 3\n    from    to access\n   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1     1     1      1\n 2     1   220      1\n 3     1   555      1\n 4     1   671      1\n 5     1   831      1\n 6     1   860      1\n 7     2     2      1\n 8     2   115      1\n 9     2   173      1\n10     2   457      1\n# ℹ 7,549 more rows\n\n\nThen, let us add, step by step, the IRIS which are neighbors with a distance lower or equal to 2, then to 3, and so on, until 30.\n\nfor (neigh_idst in seq(2,30)) {\n  read_string &lt;- paste(\n    '../data/neighbours/all_neighbour_levels/neighbours_level_',\n    neigh_idst,\n    '.csv', \n    sep = ''\n  )\n  \n  neighbor_level &lt;- read_csv(read_string, progress = FALSE)\n  \n  # Add neighbors at current distance\n  all_neighbours &lt;- \n    all_neighbours |&gt; \n    bind_rows(neighbor_level)\n}\n\nNow, we would like to extract from the resulting object, the minimum distance from one IRIS to another, for all IRIS.\n\nneighbours_all &lt;- all_neighbours |&gt;\n  group_by(from, to) |&gt; \n  summarise(min_distance = min(access), .groups = \"drop\") |&gt; \n  left_join(\n    mapping_neighbours, \n    by = c('from' = 'index_ids')\n  ) |&gt; \n  left_join(\n    mapping_neighbours, \n    by = c('to' = 'index_ids')\n  ) |&gt; \n  select(\n    from_iris = CODE_IRIS.x, \n    to_iris = CODE_IRIS.y, \n    distance = min_distance\n  )\n\nThe result can be saved:\n\nneighbours_all |&gt; \n  write_csv('../data/neighbours/all_neighbours_paris.csv')\n\n\nneighbours_all &lt;- \n  neighbours_all |&gt; \n  mutate(\n    from_iris = as.character(from_iris),\n    to_iris = as.character(to_iris)\n  )\n\nHere is the resulting matrix:\n\nneighbours_all\n\n# A tibble: 974,169 × 3\n   from_iris to_iris   distance\n   &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;\n 1 751145620 751145620        1\n 2 751145620 751145617        3\n 3 751145620 751135102        7\n 4 751145620 751156004        7\n 5 751145620 751124624       16\n 6 751145620 751051703        7\n 7 751145620 751083104       13\n 8 751145620 751166103       10\n 9 751145620 751145511        2\n10 751145620 751145512        2\n# ℹ 974,159 more rows\n\n\nLet us compute the average selling price in each IRIS:\n\ndata_immo_agg &lt;- \n  data_clean_all |&gt; \n  group_by(CODE_IRIS) |&gt; \n  summarise(mean_sale_price = mean(pm2))\n\nLet us use this to plot the observed prices per square meter, on a choropleth map. We will display the unsmoothed version on the left and the smoothed version on the right. The smoothed version is computed using Equation 2.1. We will use here a distance \\(m=5\\).\n\nm &lt;- 5\n\nsmooth_prices &lt;- \n  neighbours_all |&gt; \n  mutate(distance = distance + 1) |&gt; \n  mutate(distance = if_else(from_iris == to_iris, 1, distance)) |&gt; \n  filter(distance &lt;= !!m) |&gt; \n  left_join(\n    data_immo_agg, \n    by = c('to_iris' = 'CODE_IRIS')\n  ) |&gt; \n  mutate(inverse_weight = 1 / distance) |&gt; \n  mutate(weighted_val = mean_sale_price * inverse_weight) |&gt; \n  drop_na()  |&gt; \n  group_by(from_iris) |&gt; \n  summarise(\n    total_weights = sum(inverse_weight), \n    total_val = sum(weighted_val)\n  ) |&gt; \n  mutate(smoothed_val = total_val / total_weights) |&gt; \n  ungroup() |&gt; \n  mutate(CODE_IRIS = from_iris) |&gt; \n  select(CODE_IRIS, smoothed_price = smoothed_val)\n\n\n\nDisplay the codes to create the Figure\n# Unsmoothed version\np_1 &lt;- \n  shapes_paris |&gt; \n  left_join(\n    data_clean_all |&gt; \n      group_by(CODE_IRIS) |&gt; \n      summarise(\n        total_observations = n(), \n        median_price = mean(pm2), \n        std_price = sd(contract_final_net_price)\n      ),\n    by = \"CODE_IRIS\"\n  ) |&gt; \n  replace_na(list(total_observations = 0)) |&gt; \n  ggplot() + \n  geom_sf(aes(fill = median_price)) + \n  scale_fill_gradientn(\n    colours = rev(terrain.colors(10)), \n    limits = c(5000,20000),\n    name = 'Median Price', \n    position = 'bottom') +\n  geom_sf(\n    data = shapes_seine, \n    fill = col_seine\n  ) + \n  global_theme() + \n  guides(\n    fill = guide_colorbar(\n      title = \"Mean prices per IRIS\",\n      position = 'bottom',\n      title.position = \"top\", \n      title.vjust = 0,\n      # draw border around the legend\n      frame.colour = \"black\",\n      barwidth = 10,\n      barheight = 1.5\n    )\n  )\n\n# Smoothed version\np_2 &lt;- \n  shapes_paris |&gt; \n  left_join(smooth_prices, by = \"CODE_IRIS\") |&gt; \n  ggplot() + \n  geom_sf(aes(fill = smoothed_price)) + \n  scale_fill_gradientn(\n    colours = rev(terrain.colors(10)), \n    position = 'bottom', \n    labels = scales::comma\n  ) +\n  geom_sf(\n    data = shapes_seine, \n    fill = col_seine\n  ) + \n  global_theme()\n\n\n\n3510\n\n\n\n\n\n\nSquare meter prices by IRIS (raw values on the left, smoothed values on the right), corresponding roughly to the wealth level of the inhabitants.\n\n\n\n\n\n\n\n\n\n\n\nSquare meter prices by IRIS (raw values on the left, smoothed values on the right), corresponding roughly to the wealth level of the inhabitants.\n\n\n\n\n\n\n\n\n\n\n\nSquare meter prices by IRIS (raw values on the left, smoothed values on the right), corresponding roughly to the wealth level of the inhabitants.\n\n\n\n\n\n\n\n\nFirst, we compute the median observed price in each IRIS of the dataset, using the raw values. Then, we add the smoothed prices computed earlier (using \\(m=\\) 5), for each IRIS. Then, we aggregate the values at the arrondissement level (column NOM_COM in the map data): we compute the average of the median IRIS prices and the average of the smoothed version.\n\ndata_smooth_arrond &lt;- \n  data_clean_all |&gt; \n  mutate(CODE_IRIS = as.character(iris_cog)) |&gt; \n  group_by(CODE_IRIS) |&gt; \n  summarise(\n    median_qmp = median(pm2)\n  ) |&gt; \n  left_join(\n    smooth_prices |&gt; \n      mutate(smoothed_qmp = smoothed_price),\n    by = \"CODE_IRIS\"\n  ) |&gt; \n  left_join(\n    shapes_paris |&gt; \n      as_tibble() |&gt; \n      select(CODE_IRIS, NOM_COM),\n    by = \"CODE_IRIS\"\n  ) |&gt; \n  group_by(NOM_COM) |&gt; \n  summarise(\n    median_qmp = mean(median_qmp), \n    smoothed_qmp = mean(smoothed_qmp),\n    .groups = \"drop\"\n  )\n\nThe values are reported in Table 2.1.\n\n\nDisplay the codes used to create the Table\ndata_smooth_arrond |&gt; \n  mutate(\n    NOM_COM = factor(\n      NOM_COM, \n      levels = str_c(\n        \"Paris \", 1:20, \"e\", c(\"r\", rep(\"\", 19)), \" Arrondissement\")\n    )\n  ) |&gt; \n  arrange(NOM_COM) |&gt; \n  knitr::kable(\n    booktabs = TRUE, \n    col.names = c(\"Arrondissement\", \"Average of Median\", \"Average of Smooth\")\n  )\n\n\n\n\nTable 2.1: Aggregated observed prices at the arrondissement level.\n\n\nArrondissement\nAverage of Median\nAverage of Smooth\n\n\n\n\nParis 1er Arrondissement\n12695.811\n12483.324\n\n\nParis 2e Arrondissement\n11724.438\n11669.543\n\n\nParis 3e Arrondissement\n12525.939\n11654.372\n\n\nParis 4e Arrondissement\n13216.678\n12315.130\n\n\nParis 5e Arrondissement\n12658.348\n12068.673\n\n\nParis 6e Arrondissement\n14251.791\n13327.016\n\n\nParis 7e Arrondissement\n14233.646\n13141.138\n\n\nParis 8e Arrondissement\n12087.873\n11528.212\n\n\nParis 9e Arrondissement\n11237.138\n10849.159\n\n\nParis 10e Arrondissement\n10114.682\n10124.534\n\n\nParis 11e Arrondissement\n10271.923\n10267.033\n\n\nParis 12e Arrondissement\n9700.242\n9744.037\n\n\nParis 13e Arrondissement\n8999.515\n9521.970\n\n\nParis 14e Arrondissement\n10301.420\n10464.133\n\n\nParis 15e Arrondissement\n10283.063\n10691.530\n\n\nParis 16e Arrondissement\n11244.938\n11429.151\n\n\nParis 17e Arrondissement\n10845.312\n10917.631\n\n\nParis 18e Arrondissement\n9501.718\n9670.950\n\n\nParis 19e Arrondissement\n8334.067\n8566.154\n\n\nParis 20e Arrondissement\n8663.208\n8915.073\n\n\n\n\n\n\nLet us now visualize these values on two choropleth map, one for each aggregated method (Figure 2.3).\n\n\nDisplay the codes used to create the Figure\np_1 &lt;- shapes_paris |&gt; \n  group_by(NOM_COM) |&gt; \n  summarise(\n    arrond_shape = st_combine(geometry)\n  ) |&gt; \n  left_join(data_smooth_arrond, by = \"NOM_COM\") |&gt; \n  ggplot() + \n  geom_sf(aes(fill = median_qmp)) + \n  scale_fill_gradientn(\n    colours = rev(terrain.colors(10)), \n    limits = c(5000, 15000),\n    name = 'Price', \n    position = 'bottom', \n    labels = scales::comma\n  ) +\n  geom_sf(\n    data = shapes_seine, \n          fill = col_seine\n  ) + \n  ggtitle('Aggregated Raw') + \n  global_theme() + \n  guides(\n    fill = guide_colorbar(\n      title = \"Mean prices per Arrondissement\",\n      position='bottom',\n      title.position = \"top\", title.vjust = 0,\n      frame.colour = \"black\",\n      barwidth = 15,\n      barheight = 1.5\n    )\n  )\n\np_2 &lt;- shapes_paris |&gt; \n   group_by(NOM_COM) |&gt; \n   summarise(\n     arrond_shape = st_combine(geometry)\n   ) |&gt; \n   left_join(data_smooth_arrond, by = \"NOM_COM\") |&gt; \n   ggplot() + \n   geom_sf(aes(fill = smoothed_qmp)) + \n  scale_fill_gradientn(\n    colours = rev(terrain.colors(10)), \n    limits = c(5000, 15000),\n    name = 'Price', \n    position = 'bottom', \n    labels = scales::comma\n  ) +\n  geom_sf(\n    data = shapes_seine, \n    fill = col_seine\n  ) + \n  ggtitle('Aggregated Smoothed') + \n  global_theme()\n\nggpubr::ggarrange(\n  p_1, p_2, \n  ncol = 2,nrow = 1,\n  legend = \"bottom\", \n  common.legend = TRUE\n)\n\n\n\n\nFigure 2.3: Re-Aggregated data, left pane, mean per arrondissement when the raw, un-smoothed data is used to calculate the average price per square meter of real estate. Right pane, results when the neighbor-smoothed estimates are used."
  },
  {
    "objectID": "neigh-smoothing.html#sec-spatial-rel-error",
    "href": "neigh-smoothing.html#sec-spatial-rel-error",
    "title": "2  Neighborhood-Based Smoothing",
    "section": "2.4 Relative Errors in Prices",
    "text": "2.4 Relative Errors in Prices\nLet us dig into the relative error between estimated prices and observed ones. First, we compute the mean relative difference between the observed and the estimated price, per IRIS. The is computed as follows \\[\\frac{z - \\hat{z}}{z}\\]\n\ndiffs_smooth &lt;- \n  data_clean_all |&gt; \n  filter(pm2 &lt; 20000) |&gt; \n  filter(pm2_estimated &lt; 20000) |&gt; \n  select(CODE_IRIS, pm2, difference_pm2) |&gt; \n  mutate(relative_error = difference_pm2/pm2) |&gt; \n  group_by(CODE_IRIS) |&gt;  \n  summarise(\n    mean_relative_diff = mean(relative_error), \n    median_relative_diff = median(relative_error),\n    .groups = \"drop\"\n  )\n\nThere are two outliers, so we decide to trim the data.\n\ncuts_ &lt;- quantile(diffs_smooth$mean_relative_diff, c(0.02,0.98))\ndiffs_smooth &lt;- \n  diffs_smooth |&gt; \n  filter(mean_relative_diff &gt; cuts_[1]) |&gt; \n  filter(mean_relative_diff &lt; cuts_[2])\n\nWe use Equation 2.1 to compute the smoothed values, using \\(m=\\) 5.\n\nsmoothed_diff &lt;- \n  neighbours_all |&gt; \n  mutate(distance = distance + 1) |&gt; \n  filter(distance &lt;= !!m) |&gt; \n  mutate(distance = if_else(from_iris == to_iris, 1, distance)) |&gt; \n  left_join(\n    diffs_smooth, \n    by = c('to_iris'='CODE_IRIS')\n  ) |&gt; \n  mutate(inverse_weight = 1 / distance) |&gt; \n  mutate(value = mean_relative_diff * inverse_weight^2) |&gt; \n  drop_na() |&gt; \n  group_by(from_iris) |&gt; \n  summarise(\n    total_weights = sum(inverse_weight), \n    total_value = sum(value)\n  ) |&gt; \n  mutate(smoothed_diff = total_value / total_weights) |&gt; \n  ungroup() |&gt; \n  mutate(CODE_IRIS = from_iris, smoothed_diff)\n\nBefore plotting the map with the values, we isolate some regions of interest that will be plotted on the maps as well and used as examples in paper.\n\ntwo IRIS: Montmartre and Champs-de-Mars.\ntwo arrondissements: the 12th and the 20th.\n\n\nshape_champs &lt;- \n  shapes_paris |&gt; \n  filter(CODE_IRIS == '751072812')\n\nshape_mont &lt;- \n  shapes_paris|&gt; \n  filter(CODE_IRIS == '751093503')\n\nshape_12 &lt;- \n  shapes_paris |&gt; \n  filter(NOM_COM == 'Paris 12e Arrondissement') |&gt; \n  summarise(shape = st_union(geometry))\n\nshape_20 &lt;- \n  shapes_paris |&gt; \n  filter(NOM_COM == 'Paris 20e Arrondissement') |&gt; \n  summarise(shape = st_union(geometry))\n\nThe relative errors per IRIS are shown in Figure 2.4.\n\n\nDisplay the codes used to create the Figure\nscale_val &lt;- seq(\n  min(smoothed_diff$smoothed_diff), \n  max(smoothed_diff$smoothed_diff), \n  length.out = 4\n)\np &lt;- shapes_paris |&gt; \n  left_join(smoothed_diff, by = \"CODE_IRIS\") |&gt; \n  ggplot() + \n  geom_sf(aes(fill = smoothed_diff)) + \n  geom_sf(\n    data = shapes_seine, \n    fill = col_seine\n  ) + \n  geom_sf(\n    data = shape_champs, \n    color = 'black', \n    fill = alpha('white', 0),\n    lwd = 1, \n    lty = 'solid'\n  ) + \n  geom_sf(\n    data = shape_mont, \n    color = 'black', \n    lwd = 1, \n    fill = alpha('white', 0),\n    lty = 'solid'\n  ) + \n  geom_sf(\n    data = shape_12, \n    color = 'black', \n    lwd = 1, \n    fill = alpha('white', 0),\n    lty = 'dashed'\n  ) + \n  geom_sf(\n    data = shape_20, \n    color = 'black', \n    lwd = 1, \n    fill = alpha('white', 0),\n    lty = 'dashed'\n  ) + \n  global_theme() + \n  scale_fill_gradient2(\n    NULL,\n    midpoint = 0,\n    high = \"#000090\",\n    mid = \"white\",\n    low = \"#CD3700\",\n    breaks = scale_val,\n    labels = scales::percent(scale_val)\n  ) + \n  theme(\n    legend.position = 'bottom'\n  ) +\n  ggtitle('Smoothed relative error')\n\npanel_width &lt;- unit(1,\"npc\") - sum(ggplotGrob(p)[[\"widths\"]][-3])\np + guides(fill = guide_colorbar(barwidth = panel_width/2))\n\n\n\n\nFigure 2.4: Relative estimation error per \\(m^2\\) in different sub-regions. The values are smoothed across spatial neighbors to emphasize the spatial correlation."
  },
  {
    "objectID": "neigh-smoothing.html#sec-spatial-n-smooth",
    "href": "neigh-smoothing.html#sec-spatial-n-smooth",
    "title": "2  Neighborhood-Based Smoothing",
    "section": "2.5 Number of observation per IRIS",
    "text": "2.5 Number of observation per IRIS\nLet us also show the number of observation, using the smoothed values. First, we smooth the number of observation in each IRIS:\n\nsmooth_n &lt;- \n  neighbours_all |&gt; \n  mutate(distance = distance + 1) |&gt; \n  mutate(distance = if_else(from_iris == to_iris, 1, distance)) |&gt; \n  filter(distance &lt;= !!m) |&gt; \n  left_join(\n    data_clean_all |&gt; count(CODE_IRIS, name = \"n\"),\n    by = c('to_iris' = 'CODE_IRIS')\n  ) |&gt; \n  mutate(inverse_weight = 1 / distance) |&gt; \n  mutate(weighted_val = n * inverse_weight) |&gt; \n  drop_na()  |&gt; \n  group_by(from_iris) |&gt; \n  summarise(\n    total_weights = sum(inverse_weight), \n    total_val = sum(weighted_val)\n  ) |&gt; \n  mutate(smoothed_n = total_val / total_weights) |&gt; \n  ungroup() |&gt; \n  mutate(CODE_IRIS = from_iris) |&gt; \n  select(CODE_IRIS, smoothed_n = smoothed_n)\n\nThen, we can plot the result (Figure 2.5).\n\n\nDisplay the codes used to create the Figure\nshapes_paris |&gt; \n  left_join(smooth_n, by = \"CODE_IRIS\") |&gt; \n  ggplot() + \n  geom_sf(aes(fill = smoothed_n)) + \n  scale_fill_gradientn(\n    colours = rev(terrain.colors(10)), \n    name = 'No. Observations', \n    position = 'bottom'\n  ) +\n  geom_sf(\n    data = shapes_seine, \n    fill = col_seine\n  ) + \n  global_theme()\n\n\n\n\nFigure 2.5: Number of available observations, smoothed using the neighborhood method."
  },
  {
    "objectID": "neigh-smoothing.html#sec-neigh-wealth-iris",
    "href": "neigh-smoothing.html#sec-neigh-wealth-iris",
    "title": "2  Neighborhood-Based Smoothing",
    "section": "2.6 Wealth Level per IRIS",
    "text": "2.6 Wealth Level per IRIS\nLet us have a look at the wealth level per IRIS, using the smoothed version.\nWe load the income data per IRIS (see Section 1.4 in Chapter 1):\n\ndata_income &lt;- read_delim(\n  str_c(\n    \"../data/econ/\", \n    \"BASE_TD_FILO_DISP_IRIS_2020_CSV/BASE_TD_FILO_DISP_IRIS_2020.csv\"\n  ),\n  delim = \";\",\n  escape_double = FALSE,\n  trim_ws = TRUE\n)\n\nThe data needs to be formatted to match with the other files used here.\n\nmedian_inc_data &lt;- \n  data_income |&gt; \n  select(CODE_IRIS = IRIS, DISP_MED20) |&gt; \n  mutate(DISP_MED20 = as.numeric(DISP_MED20))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `DISP_MED20 = as.numeric(DISP_MED20)`.\nCaused by warning:\n! NAs introduced by coercion\n\n\nWe can compute the smoothed values per IRIS.\n\nsmoothed_income &lt;- \n  neighbours_all |&gt; \n  mutate(distance = distance + 1) |&gt; \n  mutate(distance = if_else(from_iris == to_iris, 1, distance)) |&gt; \n  filter(distance &lt;= !!m) |&gt; \n  left_join(\n    mapping_neighbours, \n    by=c('to_iris' = 'CODE_IRIS')\n  ) |&gt; \n  mutate(CODE_IRIS = as.character(to_iris)) |&gt; \n  left_join(\n    median_inc_data, \n    by = 'CODE_IRIS'\n  ) |&gt; \n  mutate(inverse_weight = 1 / distance) |&gt; \n  mutate(weighted_inc = DISP_MED20 * inverse_weight) |&gt; \n  drop_na() |&gt; \n  group_by(from_iris) |&gt; \n  summarise(\n    total_weights = sum(inverse_weight), \n    total_inc = sum(weighted_inc)\n  ) |&gt; \n  mutate(smoothed_inc = total_inc / total_weights) |&gt; \n  ungroup() |&gt; \n  mutate(CODE_IRIS = from_iris) |&gt; \n  select(CODE_IRIS, smoothed_income = smoothed_inc)\n\nThen, we can plot the result (Figure 2.6).\n\n\nDisplay the codes used to create the Figure\nshapes_paris |&gt; \n  left_join(smoothed_income, by = \"CODE_IRIS\") |&gt; \n  ggplot() + \n  geom_sf(aes(fill = smoothed_income)) + \n  scale_fill_gradientn(\n    colours = rev(terrain.colors(10)), \n    labels = scales::comma\n  ) +\n  geom_sf(\n    data = shapes_seine, \n    fill = col_seine\n  ) + \n  global_theme() +\n  theme(legend.position = \"bottom\") +\n  guides(\n    fill = guide_colorbar(\n      title = \"Smoothed Median Income per IRIS\",\n      position='bottom',\n      title.position = \"top\", \n      title.vjust = 0,\n      frame.colour = \"black\",\n      barwidth = 15,\n      barheight = 1.5\n    )\n  )\n\n\n\n\nFigure 2.6: Estimated Income per IRIS region, smoothed using the neighborhood method."
  },
  {
    "objectID": "model-calibration.html#load-data",
    "href": "model-calibration.html#load-data",
    "title": "3  Model Calibration",
    "section": "3.1 Load Data",
    "text": "3.1 Load Data\nLet us load the real estate data that were cleaned in Section 1.5, in Chapter 1.\n\nload(\"../data/data_clean_all.rda\")\n\nLet us have a look at the quantiles of the observed and the estimated prices. We consider here levels from 0 to 1 in steps of .05 for the quantiles.\n\nprobs &lt;- seq(0, 1, .05)\nquantile_data &lt;- tibble(\n  true_price_quant = quantile(data_clean_all$pm2, probs = probs), \n  estim_price_quant = quantile(data_clean_all$pm2_estimated, probs = probs)\n)\n\n\n\nDisplay the codes used to create the Figure.\nggplot(\n  data = quantile_data, \n  mapping = aes(x = estim_price_quant, y = true_price_quant)\n) + \n  geom_point(color = \"lightblue\", alpha = 0.8) +\n  geom_abline(color=\"violetred\", alpha = 0.5) +\n  labs(\n    x = \"Quantiles of estimated square meter prices\",\n    y = \"Quantiles of observed square meter prices\",\n  ) +\n  scale_x_continuous(labels = scales::label_comma()) +\n  scale_y_continuous(labels = scales::label_comma()) +\n  global_theme()\n\n\n\n\nFigure 3.1: Quantiles of observed prices as a function of quantiles of estimated prices. The violet line is the bissector."
  },
  {
    "objectID": "model-calibration.html#sec-calib-illustrative",
    "href": "model-calibration.html#sec-calib-illustrative",
    "title": "3  Model Calibration",
    "section": "3.2 Illustrative Example",
    "text": "3.2 Illustrative Example\nLet us consider an illustrative example, where we discretize the data into 5 classes.\n\nk &lt;- 5\n\nLet us store the observed and predicted prices in two vectors.\n\nobs &lt;- data_clean_all$pm2\npred &lt;- data_clean_all$pm2_estimated\n\nBased on the predicted prices \\(\\hat{Y}\\), we define 5 bins:\n\nquantiles &lt;- quantile(pred, probs = (0:k) / k)\nquantiles\n\n       0%       20%       40%       60%       80%      100% \n 3653.324  8958.131  9849.044 10682.828 11821.381 19927.113 \n\n\nWe compute the midpoint of each bin:\n\nmidpoints &lt;- quantiles |&gt; rollmean(quantiles, k = 2)\nmidpoints &lt;- midpoints[-length(midpoints)]\nmidpoints\n\n       0%       20%       40%       60%       80% \n 6305.728  9403.588 10265.936 11252.105 15874.247 \n\n\nNext, we compute the length of each bin, which corresponds to the distance between quantiles:\n\ndist_quantiles &lt;- diff(quantiles) # to normalize deviation\ndist_q &lt;- tibble(\n  pred_labs = 1:length(dist_quantiles), \n  dist = dist_quantiles\n) |&gt; \n  mutate(pred_labs = pred_labs)\ndist_q\n\n# A tibble: 5 × 2\n  pred_labs  dist\n      &lt;int&gt; &lt;dbl&gt;\n1         1 5305.\n2         2  891.\n3         3  834.\n4         4 1139.\n5         5 8106.\n\n\nThen, for each observed price and each predicted price, we can assign the bin in which it lies.\n\n# Change first and last values of quantiles (0% - 100%)\nquantiles[1] &lt;- 0\nquantiles[length(quantiles)] &lt;- Inf\n# Get true and predicted labels based on quantiles\nobs_labels &lt;- cut(\n  obs, breaks = quantiles, labels = 1:(length(quantiles) - 1)\n) |&gt; \n  as.numeric()\npred_labels &lt;- cut(\n  pred, breaks = quantiles, labels = 1:(length(quantiles)-1)\n) |&gt; \n  as.numeric()\n\nLet us put the results in a tibble:\n\ndata_to_display &lt;- tibble(\n  obs = obs,\n  pred = pred,\n  obs_labs = obs_labels, \n  pred_labs = pred_labels\n) |&gt;\n  left_join(dist_q, by=\"pred_labs\")\ndata_to_display\n\n# A tibble: 11,169 × 5\n      obs   pred obs_labs pred_labs  dist\n    &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1  9500   9227.        2         2  891.\n 2 14210  14481.        5         5 8106.\n 3 11314. 13481.        4         5 8106.\n 4  9838.  8553.        2         1 5305.\n 5  5294.  7132.        1         1 5305.\n 6  5042.  8035.        1         1 5305.\n 7  8684.  8708.        1         1 5305.\n 8  8167.  8931.        1         1 5305.\n 9  7647.  9946.        1         3  834.\n10  5581.  7339.        1         1 5305.\n# ℹ 11,159 more rows\n\n\n\n3.2.1 Calibration Curve\nNow, in each bin, we can compute the mean of observed and predicted prices (obs_mean and pred_mean). This will allow us to create a calibration curve. We also compute the number of observation in the bin (nb), the length of the bin (dist), the midpoints of the two quantiles defining the bin (midpoints), and the difference between the average observed price and the average predicted price (diff).\n\nlocal_means &lt;- data_to_display |&gt; \n  group_by(pred_labs) |&gt; \n  summarise(\n    obs_mean = mean(obs),\n    pred_mean = mean(pred),\n    nb = n(),\n    dist = min(dist)\n  ) |&gt;\n  mutate(midpoints = midpoints) |&gt;\n  mutate(diff = obs_mean - pred_mean)\nlocal_means\n\n# A tibble: 5 × 7\n  pred_labs obs_mean pred_mean    nb  dist midpoints   diff\n      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n1         1    8535.     8121.  2234 5305.     6306.  414. \n2         2    9687.     9421.  2234  891.     9404.  267. \n3         3   10363.    10257.  2233  834.    10266.  106. \n4         4   11202.    11191.  2234 1139.    11252.   10.9\n5         5   13149.    13412.  2234 8106.    15874. -263. \n\n\nLet us draw the calibration curve. We can define a color for each bin (we generate 10 but will only use 5 here).\n\ncolours &lt;- RColorBrewer::brewer.pal(10, \"Paired\")\nnames(colours) &lt;- seq_len(max(10))\n\nThe calibration curve is shown in Figure 3.2. If the model was perfectly calibrated, the dots would all be on the bissector.\n\n\nDisplay the codes used to create the Figure.\nx_lim &lt;- c(-500, 25000)\nggplot() + \n  geom_point(\n    data = data_to_display |&gt; \n      mutate(pred_labs = factor(pred_labs)), \n    mapping = aes(\n      x = pred, y = obs, \n      color = pred_labs\n    ), \n    alpha = 0.3\n  ) +\n  scale_colour_manual(values = colours[1:k]) +\n  # Mean point in each bin\n  geom_point(\n    data = local_means, \n    mapping = aes(x = pred_mean, y = obs_mean)\n  ) +\n  # in thousand Euros\n  labs(\n    x = \"Estimated square meter prices\",\n    y = \"Observed square meter prices\"\n  ) +\n  scale_x_continuous(\n    labels = scales::label_comma(scale = 1/1000), limits = x_lim\n  ) +\n  scale_y_continuous(\n    labels = scales::label_comma(scale = 1/1000), limits = x_lim\n  ) +\n  geom_abline(color=\"black\", alpha = 0.5) + \n  guides(color = \"none\") + \n  global_theme()\n\n\n\n\nFigure 3.2: Calibration on the whole dataset estimated using 5 bins. Prices are in thousand Euros.\n\n\n\n\n\nWe can also use a barplot to visualize how far the average observed prices are from the average predicted prices in each bin.\n\n\nDisplay the codes used to create the Figure.\ny_lim_bar &lt;- range(local_means$diff)\ny_lim_bar[1] &lt;- round(y_lim_bar[1]) - 100\ny_lim_bar[2] &lt;- round(y_lim_bar[2]) + 100\nggplot(\n  data = local_means,\n  mapping = aes(x = pred_mean, y = diff, fill = factor(pred_labs))\n) +\n  geom_bar(stat = \"identity\") +\n  geom_segment(\n    mapping = aes(\n      x = min(pred_mean), \n      y = 0, \n      xend = max(pred_mean),\n      yend = 0),\n    colour = \"black\",\n    alpha = 0.1,\n    linewidth = .1\n  ) +\n  scale_fill_manual(values = colours[1:k]) +\n  scale_x_continuous(\n    labels = scales::label_comma(scale = 1/1000), limits = x_lim\n  ) +\n  scale_y_continuous(\n    labels = scales::label_comma(scale = 1/1000), limits = y_lim_bar\n  ) +\n  guides(fill = \"none\") + \n  # in thousand Euros\n  labs(x = \"Estimated square meter prices\",, y = \"Difference\") +\n  global_theme() +\n  theme(\n    panel.grid.minor = element_blank(),\n    axis.text.x = element_blank(), \n    axis.ticks.x = element_blank()\n  )\n\n\n\n\nFigure 3.3: Calibration on the whole dataset estimated using 5 bins: difference between the average observed price and the average predicted price, in each bin. Prices are in thousand Euros.\n\n\n\n\n\n\n\n3.2.2 Expected Calibration Error\nNow, let us compute the Expected Calibration Error.\n\nn_obs &lt;- nrow(data_clean_all)\n\nThis time, we need to partition the data according to the quantiles computed on the observed prices.\n\nquantiles &lt;- quantile(data_clean_all$pm2, probs = (0:k) / k)\nquantiles\n\n         0%         20%         40%         60%         80%        100% \n   11.97183  8893.48074  9941.87479 10829.77717 12122.78446 19994.28735 \n\n\nThe length of each bin:\n\ndist_quantiles &lt;- diff(quantiles)\ndist_quantiles\n\n      20%       40%       60%       80%      100% \n8881.5089 1048.3940  887.9024 1293.0073 7871.5029 \n\n\nAnd the midpoints:\n\nmidpoints &lt;- quantiles %&gt;% rollmean(quantiles, k = 2)\nmidpoints &lt;- midpoints[-length(midpoints)]\nmidpoints\n\n       0%       20%       40%       60%       80% \n 4452.726  9417.678 10385.826 11476.281 16058.536 \n\n\nLet us get the true and predicted labels based on these quantiles:\n\n# Change first and last values of quantiles (0% - 100%)\nquantiles[1] &lt;- 0\nquantiles[length(quantiles)] &lt;- Inf\nobs_labels &lt;- as.numeric(\n  cut(obs, breaks = quantiles, labels = 1:(length(quantiles)-1))\n)\npred_labels &lt;- as.numeric(\n  cut(pred, breaks = quantiles, labels = 1:(length(quantiles)-1))\n)\n\nLet us put those values in a tibble:\n\ndata_to_display &lt;- tibble(\n  obs = obs,\n  pred = pred,\n  obs_labs = obs_labels, \n  pred_labs = pred_labels\n)\ndata_to_display\n\n# A tibble: 11,169 × 4\n      obs   pred obs_labs pred_labs\n    &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1  9500   9227.        2         2\n 2 14210  14481.        5         5\n 3 11314. 13481.        4         5\n 4  9838.  8553.        2         1\n 5  5294.  7132.        1         1\n 6  5042.  8035.        1         1\n 7  8684.  8708.        1         1\n 8  8167.  8931.        1         2\n 9  7647.  9946.        1         3\n10  5581.  7339.        1         1\n# ℹ 11,159 more rows\n\n\nWe can then compute the distances between the predicted prices and the midpoints. First, we initiate a matrix which will contain the distances.\n\ndist_y_hat &lt;- matrix(data = NA, nrow = n_obs, ncol = length(midpoints))\n\nFor each example in the data, we will compute the distance of the predicted price to each of the midpoints of the quantile-defined bins. Before doing so, let us show an example with the first observation. The predicted price is:\n\ndata_to_display$pred[1]\n\n[1] 9227.013\n\n\nIts predicted bin is:\n\ndata_to_display$pred_labs[1]\n\n[1] 2\n\n\nTo compute the distance between an observation and a midpoint of a bin, we create a function, distance_metric().\n\n#' Calculates distance between a data point and a midpoint by normalizing to 1\n#' distance between quantiles\n#' \n#' @param x observation\n#' @param x_class assigned bin class\n#' @param midpoint midpoint midpoints of the bin of interest\n#' @param midpoints midpoints of all the bins\n#' @param quantiles quantiles defining the bins\n#' @param dist_quantiles length of each bin\ndistance_metric &lt;- function(x, \n                            x_class, \n                            midpoint, \n                            midpoints,\n                            quantiles,\n                            dist_quantiles\n                            ) {\n  midpoint_class &lt;- which(midpoints == midpoint)\n  if (x_class == midpoint_class){\n    return(distance(x, midpoint) / dist_quantiles[x_class])\n  } else if (x_class &gt; midpoint_class){\n    diff_class &lt;- x_class - midpoint_class\n    dist &lt;- rep(NA, 2)\n    # First and last distance values\n    dist[1] &lt;- 0.5\n    dist[length(dist)] &lt;- \n      distance(quantiles[x_class], x) / dist_quantiles[x_class]\n    dist &lt;- sum(dist)\n    if (diff_class &gt; 1) {\n      dist &lt;- dist + (diff_class - 1)\n    }\n    return(dist)\n  } else {\n    diff_class &lt;- midpoint_class - x_class\n    dist &lt;- rep(NA, 2)\n    # First and last distance values\n    dist[1] &lt;- \n      distance(quantiles[x_class + 1], x) / dist_quantiles[x_class]\n    dist[length(dist)] &lt;- 0.5\n    dist &lt;- sum(dist)\n    if (diff_class &gt; 1) {\n      dist &lt;- dist + (diff_class - 1)\n    }\n    return(dist)\n  }\n}\n\nThis function needs another one, distance(), that returns the Euclidean distance between two points in \\(\\mathbb{R}\\).\n\ndistance &lt;- function(a, b) {\n  return(abs(a - b))\n}\n\nThe distance to the j-th midpoint can be computed as follows:\n\nj &lt;- 1\ndistance_metric(\n  x = data_to_display$pred[1], \n  x_class = data_to_display$pred_labs[1], \n  midpoint = midpoints[j],\n  midpoints = midpoints,\n  quantiles = quantiles,\n  dist_quantiles = dist_quantiles\n)\n\n[1] 0.8181359\n\n\nLet us apply this distance_metric() function to all the observations, and for each observation, to all the midpoints.\n\nfor (i in 1:n_obs) {\n  for (j in 1:length(midpoints)) {\n    dist_y_hat[i,j] &lt;- distance_metric(\n      x = data_to_display$pred[i], \n      x_class = data_to_display$pred_labs[i], \n      midpoint = midpoints[j],\n      midpoints = midpoints,\n      quantiles = quantiles,\n      dist_quantiles = dist_quantiles\n    )\n  }\n}\n\nThe predicted prices for the 10 first observations are:\n\ndata_to_display$pred[1:10]\n\n [1]  9227.013 14480.945 13481.491  8553.486  7132.463  8034.759  8708.281\n [8]  8931.178  9945.561  7338.527\n\n\nAnd the distances to each midpoint:\n\ndist_y_hat[1:10,]\n\n           [,1]      [,2]      [,3]      [,4]      [,5]\n [1,] 0.8181359 0.1818641 1.1818641 2.1818641 3.1818641\n [2,] 3.7995820 2.7995820 1.7995820 0.7995820 0.2004180\n [3,] 3.6726109 2.6726109 1.6726109 0.6726109 0.3273891\n [4,] 0.4617188 0.5382812 1.5382812 2.5382812 3.5382812\n [5,] 0.3017209 0.6982791 1.6982791 2.6982791 3.6982791\n [6,] 0.4033136 0.5966864 1.5966864 2.5966864 3.5966864\n [7,] 0.4791477 0.5208523 1.5208523 2.5208523 3.5208523\n [8,] 0.5359573 0.4640427 1.4640427 2.4640427 3.4640427\n [9,] 1.5041515 0.5041515 0.4958485 1.4958485 2.4958485\n[10,] 0.3249223 0.6750777 1.6750777 2.6750777 3.6750777\n\n\nLet us save these results (so that we can re-use them in Chapter 5).\n\nas_tibble(dist_y_hat) |&gt; \n  rename_with(~str_remove(.x, \"V\"), .cols = everything()) |&gt; \n  mutate(id = data_clean_all$id) |&gt; \n  relocate(id, .before = `1`) |&gt; \n  write_csv(\"../data/distances.csv\")\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nℹ Using compatibility `.name_repair`.\n\n\nFor each observation, we can then apply the softmax function so that the distances for each observation sum to 1. The resulting values are considered as scores.\n\nconfidence_scores &lt;- LDATS::softmax(-dist_y_hat)\nconfidence_scores[1:10,]\n\n            [,1]       [,2]      [,3]       [,4]       [,5]\n [1,] 0.25417622 0.48024657 0.1766728 0.06499431 0.02391007\n [2,] 0.01475786 0.04011601 0.1090466 0.29641945 0.53966006\n [3,] 0.01678987 0.04563959 0.1240613 0.33723345 0.47627584\n [4,] 0.41008176 0.37985677 0.1397415 0.05140802 0.01891195\n [5,] 0.48909384 0.32897976 0.1210249 0.04452257 0.01637894\n [6,] 0.43860715 0.36148886 0.1329843 0.04892220 0.01799747\n [7,] 0.40167638 0.38526911 0.1417326 0.05214050 0.01918142\n [8,] 0.37470037 0.40263935 0.1481227 0.05449131 0.02004623\n [9,] 0.12757524 0.34678546 0.3496768 0.12863890 0.04732361\n[10,] 0.47750661 0.33644095 0.1237697 0.04553233 0.01675041\n\n\nWe can then assign a predicted label based on the distance:\n\npredicted_labels &lt;- ramify::argmax(confidence_scores)\npredicted_labels[1:10]\n\n [1] 2 5 5 1 1 1 1 2 3 1\n\n\nLet us extract the maximum score for each observation:\n\nmax_confidence_scores &lt;- apply(confidence_scores, 1, max)\nmax_confidence_scores[1:10]\n\n [1] 0.4802466 0.5396601 0.4762758 0.4100818 0.4890938 0.4386071 0.4016764\n [8] 0.4026394 0.3496768 0.4775066\n\n\nThese scores can be added to the data_to_display tibble.\n\ndata_to_display &lt;- data_to_display |&gt;  \n    mutate(scores_conf = max_confidence_scores)\n\nRecall that in data_to_display, each observed price and each predicted price were assigned to a bin. The bins were defined using the quantiles computed on the predicted prices. It is possible to compare the assigned bin based on the observed or the predicted value. We can use this comparison to compute an overall accuracy.\n\ndata_to_display &lt;- data_to_display |&gt; \n    mutate(acc = if_else(obs_labs == pred_labs, 1, 0))\n\nEventually, we can compute the expected calibration error. Let us define new bins based on the predicted scores we just obtained.\n\nbreaks &lt;- quantile(data_to_display$scores_conf, probs = (0:k) / k)\nbreaks\n\n       0%       20%       40%       60%       80%      100% \n0.3483272 0.4011736 0.4274774 0.4564554 0.4893255 0.6364086 \n\n\nWe need to assign, for each score in the dataset, the corresponding bin.\n\ntb_breaks &lt;- tibble(breaks = breaks, labels = 0:k) |&gt;\n  group_by(breaks) |&gt;\n  slice_tail(n = 1) |&gt;\n  ungroup()\ntb_breaks\n\n# A tibble: 6 × 2\n  breaks labels\n   &lt;dbl&gt;  &lt;int&gt;\n1  0.348      0\n2  0.401      1\n3  0.427      2\n4  0.456      3\n5  0.489      4\n6  0.636      5\n\n\nLet us add the corresponding bin in data_to_display and save the tibble in a new one called x_with_class.\n\nx_with_class &lt;- data_to_display |&gt;\n  mutate(\n    score_bin = cut(\n      scores_conf,\n      breaks = tb_breaks$breaks,\n      labels = tb_breaks$labels[-1],\n      include.lowest = TRUE\n    )\n  )\nx_with_class\n\n# A tibble: 11,169 × 7\n      obs   pred obs_labs pred_labs scores_conf   acc score_bin\n    &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;    \n 1  9500   9227.        2         2       0.480     1 4        \n 2 14210  14481.        5         5       0.540     1 5        \n 3 11314. 13481.        4         5       0.476     0 4        \n 4  9838.  8553.        2         1       0.410     0 2        \n 5  5294.  7132.        1         1       0.489     1 4        \n 6  5042.  8035.        1         1       0.439     1 3        \n 7  8684.  8708.        1         1       0.402     1 2        \n 8  8167.  8931.        1         2       0.403     0 2        \n 9  7647.  9946.        1         3       0.350     0 1        \n10  5581.  7339.        1         1       0.478     1 4        \n# ℹ 11,159 more rows\n\n\nIn each bin defined using the scores, we can compute the accuracy and the confidence:\n\nece_by_bin &lt;- x_with_class |&gt;  \n  group_by(score_bin) |&gt; \n  summarise(\n    acc = mean(acc), \n    conf = mean(scores_conf), \n    nb = n()\n  )\nece_by_bin\n\n# A tibble: 5 × 4\n  score_bin   acc  conf    nb\n  &lt;fct&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 1         0.358 0.381  2234\n2 2         0.434 0.414  2234\n3 3         0.477 0.442  2233\n4 4         0.468 0.473  2234\n5 5         0.581 0.526  2234\n\n\nLastly, we can compute the expected calibration error:\n\nece &lt;- sum((ece_by_bin$nb / n_obs) * abs(ece_by_bin$acc - ece_by_bin$conf))\nece\n\n[1] 0.02768918"
  },
  {
    "objectID": "model-calibration.html#helper-functions",
    "href": "model-calibration.html#helper-functions",
    "title": "3  Model Calibration",
    "section": "3.3 Helper Functions",
    "text": "3.3 Helper Functions\nFor convenience, let us wrap the code from the illustrative example into some helper functions.\n\n3.3.1 Quantile Binning\nFirst, we are going to build a function that will:\n\npartition the interval \\([0,1]\\) into \\(B\\) bins, based on quantiles of the estimated prices, where each bin \\(b\\in[B]\\) is associated with the set \\(\\mathcal{I}_b\\) containing the indices of instances within that bin,\nassign to each observed and predicted values the corresponding bin it lies in, and also return the middle of value of the corresponding bin,\n\n\n#' Partition predicted values into k bins\n#' \n#' @param obs vector of observed values (y)\n#' @param pred vector of predicted values (\\hat{y})\n#' @param k number of bins\n#' @param partition_along which vector of values to use to partition \n#' (`\"obs\"` or `\"pred`)\n#' \n#' @returns a list with four elements:\n#'  - `quantiles`: the empirical quantiles of the predicted values\n#'  - `midpoints`: the midpoint of each bin\n#'  - `dist_quantiles`: the width of each bin\n#'  - `data_to_display`: a tibble with the observed and predicted values, as well\n#'    as their predicted bins (`obs_labs` and `pred_labs`), and the middle of\n#'    the bin (`dist`)\nquantile_binning &lt;- function(obs, \n                             pred, \n                             k,\n                             partition_along = c(\"obs\", \"pred\")\n                             ) {\n  \n  if (partition_along == \"obs\") {\n    # Define k bins on Y (quantile)\n    quantiles &lt;- quantile(obs, probs = (0:k) / k)\n  } else {\n    # Define k bins on \\hat_{Y} (quantile)\n    quantiles &lt;- quantile(pred, probs = (0:k) / k)\n  }\n  \n  # Midpoints of those quantiles\n  midpoints &lt;- quantiles |&gt; rollmean(quantiles, k = 2)\n  midpoints &lt;- midpoints[-length(midpoints)]\n  # Distance between quantiles (length of each bin)\n  dist_quantiles &lt;- diff(quantiles) # to normalize deviation\n  dist_q &lt;- tibble(\n    pred_labs = 1:length(dist_quantiles), \n    dist = dist_quantiles\n  ) |&gt; \n    mutate(pred_labs = pred_labs)\n  # Change first and last values of quantiles (0% - 100%)\n  quantiles[1] &lt;- 0\n  quantiles[length(quantiles)] &lt;- Inf\n  # Get true and predicted labels based on quantiles\n  obs_labels &lt;- cut(\n    obs, breaks = quantiles, labels = 1:(length(quantiles) - 1)\n  ) |&gt; \n    as.numeric()\n  pred_labels &lt;- cut(\n    pred, breaks = quantiles, labels = 1:(length(quantiles)-1)\n  ) |&gt; \n    as.numeric()\n  data_to_display &lt;- tibble(\n    obs = obs,\n    pred = pred,\n    obs_labs = obs_labels, \n    pred_labs = pred_labels\n  ) |&gt;\n    left_join(dist_q, by=\"pred_labs\")\n  \n  list(\n    quantiles = quantiles,\n    midpoints = midpoints,\n    dist_quantiles = dist_quantiles,\n    data_to_display = data_to_display\n  )\n}\n\nThen, we define another function, compute_local_means(), that computes for each bin the average observed values \\(E[Y|\\hat{Y} \\in [q_b ; q_{b+1}]]\\) and the average predicted values \\(E[\\hat{Y}|\\hat{Y} \\in [q_b,q_{b+1}]]\\).\n\n#' Estimate `E[Y|\\hat{Y} \\in [q_k ; q_{k+1}]]` related to\n#' `E[\\hat{Y}|\\hat{Y} \\in [q_k,q_{k+1}]]`.\n#' \n#' @param obs vector of observed values (y)\n#' @param pred vector of predicted values (\\hat{y})\n#' @param midpoints midpoints of all the bins\ncompute_local_means &lt;- function(obs, \n                                pred, \n                                data_to_display, \n                                midpoints) {\n  local_means &lt;- data_to_display |&gt; \n    group_by(pred_labs) |&gt; \n    summarise(\n      obs_mean = mean(obs),\n      pred_mean = mean(pred),\n      nb = n(),\n      dist = min(dist)\n    ) |&gt;\n    mutate(midpoints = midpoints) |&gt;\n    mutate(diff = obs_mean - pred_mean)\n  local_means\n}\n\n\n\n3.3.2 Expected Calibration Error\nFirst, we need to define a function, distance(), that returns the Euclidean distance between two points in in \\(\\mathcal{R}\\).\n\ndistance &lt;- function(a, b) {\n  return(abs(a - b))\n}\n\nSecond, we define a function to compute the distance between a data point an a midpoint. The distance between quantiles is normalized to 1.\n\n#' Calculates distance between a data point and a midpoint by normalizing to 1\n#' distance between quantiles\n#' \n#' @param x observation\n#' @param x_class assigned bin class\n#' @param midpoint midpoint midpoints of the bin of interest\n#' @param midpoints midpoints of all the bins\n#' @param quantiles quantile defining the bins\n#' @param dist_quantiles length of each bin\ndistance_metric &lt;- function(x, \n                            x_class, \n                            midpoint, \n                            midpoints,\n                            quantiles,\n                            dist_quantiles\n                            ) {\n  midpoint_class &lt;- which(midpoints == midpoint)\n  if (x_class == midpoint_class){\n    return(distance(x, midpoint) / dist_quantiles[x_class])\n  } else if (x_class &gt; midpoint_class){\n    diff_class &lt;- x_class - midpoint_class\n    dist &lt;- rep(NA, 2)\n    # First and last distance values\n    dist[1] &lt;- 0.5\n    dist[length(dist)] &lt;- \n      distance(quantiles[x_class], x) / dist_quantiles[x_class]\n    dist &lt;- sum(dist)\n    if (diff_class &gt; 1) {\n      dist &lt;- dist + (diff_class - 1)\n    }\n    return(dist)\n  } else {\n    diff_class &lt;- midpoint_class - x_class\n    dist &lt;- rep(NA, 2)\n    # First and last distance values\n    dist[1] &lt;- \n      distance(quantiles[x_class + 1], x) / dist_quantiles[x_class]\n    dist[length(dist)] &lt;- 0.5\n    dist &lt;- sum(dist)\n    if (diff_class &gt; 1) {\n      dist &lt;- dist + (diff_class - 1)\n    }\n    return(dist)\n  }\n}\n\nLastly, we need a function to compute the Expected Calibration Error.\n\n#' Function to calculate ECE\n#' \n#' @param obs vector of observed values (y)\n#' @param pred vector of predicted values (\\hat{y})\n#' @param k number of bins\ncalculate_ece &lt;- function(obs, \n                          pred, \n                          k) {\n  n_obs &lt;- length(obs)\n  \n  # Quantile binning\n  q_binning &lt;- quantile_binning(\n    obs = obs, pred = pred, k = k, partition_along = \"obs\"\n  )\n  data_to_display &lt;- q_binning$data_to_display\n  quantiles &lt;- q_binning$quantiles\n  midpoints &lt;- q_binning$midpoints\n  dist_quantiles &lt;- q_binning$dist_quantiles\n  \n  # Distance between predicted price and each midpoint of the bins\n  dist_y_hat &lt;- matrix(data = NA, nrow = n_obs, ncol = length(midpoints))\n  for (i in 1:n_obs) {\n    for (j in 1:length(midpoints)) {\n      dist_y_hat[i,j] &lt;- distance_metric(\n        x = data_to_display$pred[i], \n        x_class = data_to_display$pred_labs[i], \n        midpoint = midpoints[j],\n        midpoints = midpoints,\n        quantiles = quantiles,\n        dist_quantiles = dist_quantiles\n      )\n    }\n  }\n  \n  # Softmax applied on the distances: get the scores\n  confidence_scores &lt;- LDATS::softmax(-dist_y_hat)\n  \n  # Predicted labels\n  predicted_labels &lt;- ramify::argmax(confidence_scores)\n  \n  # Max score for each observation: confidence\n  max_confidence_scores &lt;- apply(confidence_scores, 1, max)\n  data_to_display &lt;- data_to_display |&gt;  \n    mutate(scores_conf = max_confidence_scores)\n  \n  # Accuracy\n  data_to_display &lt;- data_to_display |&gt; \n    mutate(acc = if_else(obs_labs == pred_labs, 1, 0))\n  \n  # ECE\n  # new bins based on scores\n  breaks &lt;- quantile(data_to_display$scores_conf, probs = (0:k) / k)\n  tb_breaks &lt;- tibble(breaks = breaks, labels = 0:k) |&gt;\n    group_by(breaks) |&gt;\n    slice_tail(n = 1) |&gt;\n    ungroup()\n  \n  x_with_class &lt;- data_to_display |&gt;\n    mutate(\n      score_bin = cut(\n        scores_conf,\n        breaks = tb_breaks$breaks,\n        labels = tb_breaks$labels[-1],\n        include.lowest = TRUE\n      )\n    )\n  \n  # Accuracy and Confidence in each bin\n  ece_by_bin &lt;- x_with_class |&gt;  \n    group_by(score_bin) |&gt; \n    summarise(\n      acc = mean(acc), \n      conf = mean(scores_conf), \n      nb = n()\n    )\n  \n  ece &lt;- sum((ece_by_bin$nb / n_obs) * abs(ece_by_bin$acc - ece_by_bin$conf))\n  ece\n}"
  },
  {
    "objectID": "model-calibration.html#whole-dataset",
    "href": "model-calibration.html#whole-dataset",
    "title": "3  Model Calibration",
    "section": "3.4 Whole Dataset",
    "text": "3.4 Whole Dataset\nLet us apply the quantile_binning() function to split the data into bins and compute the average of observed prices and predicted prices in each bin.\nLet us make the number of bins vary so that we consider in turn 3, 5, and 10 bins.\n\nk_values &lt;- c(3, 5, 10)\n\nWe initiate empty objects that will contain the estimated means of observed and predicted prices in each bin.\n\ndata_to_display &lt;- local_means &lt;- ece_values &lt;- \n  vector(mode = \"list\", length(k_values))\n\nWe can the loop over the different values for the number of bins to consider:\n\nfor (i in 1:length(k_values)) {\n  k &lt;- k_values[i]\n  obs &lt;- data_clean_all$pm2\n  pred &lt;- data_clean_all$pm2_estimated\n  res_q &lt;- quantile_binning(\n    obs = obs, pred = pred, k = k, partition_along = \"pred\"\n  )\n  data_to_display[[i]] &lt;- res_q$data_to_display |&gt; mutate(k = !!k)\n  \n  # For calibration curve\n  local_means_current &lt;- compute_local_means(\n    obs = obs, pred = pred, \n    data_to_display = res_q$data_to_display, \n    midpoints = res_q$midpoints\n  )\n  local_means[[i]] &lt;- local_means_current |&gt; mutate(k = !!k)\n  \n  # Calculate ECE\n  ece_current &lt;- calculate_ece(\n    obs = obs, \n    pred = pred,\n    k = k\n  )\n  ece_values[[i]] &lt;- ece_current\n}\n\nLet us bind the tibbles into a single one:\n\ndata_to_display &lt;- list_rbind(data_to_display) |&gt; as_tibble()\nlocal_means &lt;- list_rbind(local_means) |&gt; as_tibble()\n\nNow, let us visualize the calibration of the price model for the different number of bins that we envisaged. To do so, we loop over the different values for the number of bins and create a calibration graph in each case.\nLet us define some colors:\n\ncolours &lt;- RColorBrewer::brewer.pal(10, \"Paired\")\nnames(colours) &lt;- seq_len(max(10))\n\nThen we prepare the plots:\n\nplots &lt;- vector(mode = \"list\", length = length(k_values))\nfor (i in 1:length(k_values)) {\n  k &lt;- k_values[i]\n  data_to_display_k &lt;- data_to_display |&gt; filter(k == !!k)\n  local_means_k &lt;- local_means |&gt; filter(k == !!k)\n  \n  x_lim &lt;- c(-500, 25000)\n  y_lim_bar &lt;- range(local_means$diff)\n  y_lim_bar[1] &lt;- round(y_lim_bar[1]) - 100\n  y_lim_bar[2] &lt;- round(y_lim_bar[2]) + 100\n  \n  scatter_plot &lt;- ggplot() + \n    # All prices\n    geom_point(\n      data = data_to_display_k |&gt; \n        mutate(pred_labs = factor(pred_labs)), \n      mapping = aes(\n        x = pred, y = obs, \n        color = pred_labs\n      ), \n      alpha = 0.3\n    ) +\n    scale_colour_manual(values = colours[1:k]) +\n    # Mean point in each bin\n    geom_point(\n      data = local_means_k, \n      mapping = aes(x = pred_mean, y = obs_mean)\n    ) +\n    # in thousand Euros\n    labs(\n      x = \"Estimated square meter prices\",\n      y = \"Observed square meter prices\"\n    ) +\n    scale_x_continuous(\n      labels = scales::label_comma(scale = 1/1000), limits = x_lim\n    ) +\n    scale_y_continuous(\n      labels = scales::label_comma(scale = 1/1000), limits = x_lim\n    ) +\n    geom_abline(color=\"black\", alpha = 0.5) + \n    guides(color = \"none\") + \n    global_theme()\n  \n  \n  bar_plot &lt;- ggplot(\n    data = local_means_k,\n    mapping = aes(x = pred_mean, y = diff, fill = factor(pred_labs))\n  ) +\n    geom_bar(stat = \"identity\") +\n    geom_segment(\n      mapping = aes(\n        x = min(pred_mean), \n        y = 0, \n        xend = max(pred_mean),\n        yend = 0),\n      colour = \"black\",\n      alpha = 0.1,\n      linewidth = .1\n    ) +\n    scale_fill_manual(values = colours[1:k]) +\n    scale_x_continuous(\n      labels = scales::label_comma(scale = 1/1000), limits = x_lim\n    ) +\n    scale_y_continuous(\n      labels = scales::label_comma(scale = 1/1000), limits = y_lim_bar\n    ) +\n    guides(fill = \"none\") + \n    # in thousand Euros\n    labs(x = NULL, y = \"Difference\") +\n    global_theme() +\n    theme(\n      panel.grid.minor = element_blank(),\n      axis.text.x = element_blank(), \n      axis.ticks.x = element_blank()\n    )\n  \n  title_plot &lt;- \n    ggplot() + \n    labs(title = str_c(k, \" bins\\n (ECE: \", round(ece_values[[i]],3), \")\")) + \n    global_theme()\n  \n  plots[[i]] &lt;- \n    list(\n      title_plot = title_plot,\n      scatter_plot = scatter_plot,\n      bar_plot = bar_plot\n    )\n  \n}\n\nWe can then display the calibration plots. The left panel in Figure 3.4 shows the calibration when considering 3 bins, the panel in the middle is when the number of bins is 5 and the one on the right is when the number of bins is 10.\n\n\nDisplay the codes used to create the Figure.\ncowplot::plot_grid(\n  plots[[1]]$title_plot,\n  plots[[2]]$title_plot,\n  plots[[3]]$title_plot,\n  #\n  plots[[1]]$scatter_plot,\n  plots[[2]]$scatter_plot,\n  plots[[3]]$scatter_plot,\n  #\n  plots[[1]]$bar_plot,\n  plots[[2]]$bar_plot,\n  plots[[3]]$bar_plot,\n  ncol = 3,\n  rel_heights = c(1, 10, 2),\n  align = \"v\"\n)\n\n\n\n\nFigure 3.4: Calibration on the whole dataset estimated using different number of bins. Prices are in thousand Euros."
  },
  {
    "objectID": "model-calibration.html#comparison-with-a-single-arrondissement",
    "href": "model-calibration.html#comparison-with-a-single-arrondissement",
    "title": "3  Model Calibration",
    "section": "3.5 Comparison With a Single Arrondissement",
    "text": "3.5 Comparison With a Single Arrondissement\nLet us now consider 5 bins, and contrast three situations for the calibration of the model:\n\nusing the whole dataset as previously,\nusing the whole dataset but with prices estimated randomly drawn,\nfocusing on a single arrondissement.\n\n\n# Number of desired bins\nk &lt;- 5\n\n# Init. result object\ndata_to_display &lt;- local_means &lt;- ece_values &lt;- \n  vector(mode = \"list\", 3)\n\n\n3.5.1 On the Whole Dataset\n\nobs &lt;- data_clean_all$pm2\npred &lt;- data_clean_all$pm2_estimated\n\nres_q &lt;- quantile_binning(\n  obs = obs, pred = pred, k = k, partition_along = \"pred\"\n)\ndata_to_display[[1]] &lt;- res_q$data_to_display |&gt; mutate(type = \"Whole dataset\")\nlocal_means_current &lt;- compute_local_means(\n    obs = obs, \n    pred = pred,\n    data_to_display = res_q$data_to_display, \n    midpoints = res_q$midpoints\n  )\nlocal_means[[1]] &lt;- local_means_current |&gt; mutate(type = \"Whole dataset\")\n\nece_current &lt;- calculate_ece(\n    obs = obs, \n    pred = pred,\n    k = k\n  )\nece_values[[1]] &lt;- ece_current\n\n\n\n3.5.2 Random Model\nWe will draw the values for \\(\\hat{y}\\) on a \\(\\mathcal{U}[\\underline{Y}, \\overline{Y}]\\), where \\(\\underline{Y}\\) represents the minimum of the observed prices and \\(\\overline{Y}\\) represents the maximum of the observed prices.\n\nn &lt;- nrow(data_clean_all)\ny_lim &lt;- c(min(data_clean_all$pm2), max(data_clean_all$pm2))\n# Draw n values from a U[min(Y), max(Y)]\nset.seed(123)\nobs &lt;- data_clean_all$pm2\npred &lt;- runif(n, y_lim[1], y_lim[2])\nres_q &lt;- quantile_binning(\n  obs = obs, pred = pred, k = k, partition_along = \"pred\"\n)\ndata_to_display[[2]] &lt;- res_q$data_to_display |&gt; mutate(type = \"Random Model\")\nlocal_means_current &lt;- compute_local_means(\n  obs = obs, \n  pred = pred,\n  data_to_display = res_q$data_to_display, \n  midpoints = res_q$midpoints\n)\nlocal_means[[2]] &lt;- local_means_current |&gt; mutate(type = \"Random Model\")\n\nece_current &lt;- calculate_ece(\n    obs = obs, \n    pred = pred,\n    k = k\n  )\nece_values[[2]] &lt;- ece_current\n\n\n\n3.5.3 7th Arrondissement\nLet us focus on the 7th arrondissement. We first need to know which IRIS codes are in the th arrondissement. To do so, we can simply use the NOM_COM variable in the dataset.\n\ndata_7th &lt;- \n  data_clean_all |&gt; \n  filter(NOM_COM == \"Paris 7e Arrondissement\")\nnrow(data_7th)\n\n[1] 327\n\n\nLet us compute the average values of observed and predicted prices in each bins defined using the 5 quantiles of the observed prices in the 7th arrondissement.\n\nobs &lt;- data_7th$pm2\npred &lt;- data_7th$pm2_estimated\nres_q &lt;- quantile_binning(\n  obs = obs, pred = pred, k = k, partition_along = \"pred\"\n)\ndata_to_display[[3]] &lt;- res_q$data_to_display |&gt; \n  mutate(type = \"7th arrondissement\")\nlocal_means_current &lt;- compute_local_means(\n  obs = obs, \n  pred = pred,\n  data_to_display = res_q$data_to_display, \n  midpoints = res_q$midpoints\n)\nlocal_means[[3]] &lt;- local_means_current |&gt; mutate(type = \"7th arrondissement\")\n\nece_current &lt;- calculate_ece(\n    obs = obs, \n    pred = pred,\n    k = k\n  )\nece_values[[3]] &lt;- ece_current\n\n\n\n3.5.4 Visualizing the Calibration for the 3 Cases\nNow, we can create the calibration plot in each of the three situations, and plot the result.\n\nplots_compar &lt;- vector(mode = \"list\", length = 3)\nfor (i in 1:3) {\n  data_to_display_curr &lt;- data_to_display[[i]]\n  local_means_curr &lt;- local_means[[i]]\n  \n  \n  x_lim &lt;- c(0, max(data_clean_all$pm2))\n  if (data_to_display_curr$type[1] == \"Random Model\") {\n    y_lim_bar &lt;- range(local_means_curr$diff)\n  } else {\n    y_lim_bar &lt;- map(local_means[c(1,3)], \"diff\") |&gt; \n      map(range) |&gt; \n      unlist() |&gt; \n      range()\n  }\n  y_lim_bar[1] &lt;- round(y_lim_bar[1]) - 100\n  y_lim_bar[2] &lt;- round(y_lim_bar[2]) + 100 \n  \n  \n  scatter_plot &lt;- ggplot() + \n    # All prices\n    geom_point(\n      data = data_to_display_curr |&gt; \n        mutate(pred_labs = factor(pred_labs)), \n      mapping = aes(\n        x = pred, y = obs, \n        color = pred_labs\n      ), \n      alpha = 0.3\n    ) +\n    scale_colour_manual(values = colours) +\n    # Mean point in each bin\n    geom_point(\n      data = local_means_curr, \n      mapping = aes(x = pred_mean, y = obs_mean)\n    ) +\n    # in thousand Euros\n    labs(\n      x = \"Estimated square meter prices\",\n      y = \"Observed square meter prices\"\n    ) +\n    scale_x_continuous(\n      labels = scales::label_comma(scale = 1/1000), limits = x_lim\n    ) +\n    scale_y_continuous(\n      labels = scales::label_comma(scale = 1/1000), limits = x_lim\n    ) +\n    geom_abline(color=\"black\", alpha = 0.5) + \n    guides(color = \"none\") + \n    global_theme()\n  \n  bar_plot &lt;- ggplot(\n    data = local_means_curr,\n    mapping = aes(x = pred_mean, y = diff, fill = factor(pred_labs))\n  ) +\n    geom_bar(stat = \"identity\") +\n    geom_segment(\n      mapping = aes(\n        x = min(pred_mean), \n        y = 0, \n        xend = max(pred_mean),\n        yend = 0),\n      colour = \"black\",\n      alpha = 0.1,\n      linewidth = .1\n    ) +\n    scale_fill_manual(values = colours[1:k]) +\n    scale_x_continuous(\n      labels = scales::label_comma(scale = 1/1000), limits = x_lim\n    ) +\n    scale_y_continuous(\n      labels = scales::label_comma(scale = 1/1000), limits = y_lim_bar\n    ) +\n    guides(fill = \"none\") + \n    # in thousand Euros\n    labs(x = NULL, y = \"Difference\") +\n    global_theme() +\n    theme(\n      panel.grid.minor = element_blank(),\n      axis.text.x = element_blank(), \n      axis.ticks.x = element_blank()\n    )\n  \n  title &lt;- str_c(\n    unique(data_to_display_curr$type), \"\\n (ECE:\", round(ece_values[[i]], 3), \")\"\n  )\n  \n  title_plot &lt;- \n    ggplot() + \n    labs(title = title) + \n    global_theme()\n  \n  plots_compar[[i]] &lt;- \n    list(\n      title_plot = title_plot,\n      scatter_plot = scatter_plot,\n      bar_plot = bar_plot\n    )\n  \n}\n\nThe results are shown in Figure 3.5, with the calibration estimated on the whole dataset on the left panel, the calibration estimated when the estimated prices are randomly drawn in the middle panel, and the calibration estimated on the subset of observations corresponding to the 7th arrondissement only.\n\n\nDisplay the codes used to create the Figure.\ncowplot::plot_grid(\n  plots_compar[[1]]$title_plot,\n  plots_compar[[2]]$title_plot,\n  plots_compar[[3]]$title_plot,\n  #\n  plots_compar[[1]]$scatter_plot,\n  plots_compar[[2]]$scatter_plot,\n  plots_compar[[3]]$scatter_plot,\n  #\n  plots_compar[[1]]$bar_plot,\n  plots_compar[[2]]$bar_plot,\n  plots_compar[[3]]$bar_plot,\n  ncol = 3,\n  rel_heights = c(1, 10, 2),\n  align = \"v\"\n)\n\n\n\n\nFigure 3.5: Calibration on the whole dataset (left), on the observation from the 7th \u001bmph{arrondissement} only (right) and on randomly drawn values (middle); bins defined using quintiles. Prices are in thousand Euros."
  },
  {
    "objectID": "model-calibration.html#sec-ece-arrond",
    "href": "model-calibration.html#sec-ece-arrond",
    "title": "3  Model Calibration",
    "section": "3.6 ECE in each Arrondissement",
    "text": "3.6 ECE in each Arrondissement\nLet us compute the ECE in each arrondissement.\nFor convenience, let us wrap the code from Section 3.5.3 in a function, so that we can compute the ECE for a given arrondissement.\n\n#' Computes ECE for a single arrondissement, depending on the neighbors distance\n#' \n#' @param arrond name od the arrondissement\n#' @param k number of neighbors to consider\n#' @param obs_name name of the variable with observed values in the data\n#' @param pred_name name of the variable with predicted values in the data\n#' @param data dataset to use\ncompute_ece_arrondissement &lt;- function(arrond, \n                                       k, \n                                       obs_name = \"pm2\", \n                                       pred_name = \"pm2_estimated\",\n                                       data) {\n  data_current &lt;- data |&gt; filter(NOM_COM == !!arrond)\n  obs &lt;- data_current |&gt; pull(!!obs_name)\n  pred &lt;- data_current |&gt; pull(!!pred_name)\n  ece &lt;- calculate_ece(\n    obs = obs, \n    pred = pred,\n    k = k\n  )\n  ece\n}\n\nWe just need to loop over the names of the different arrondissements to calculate the expected calibration errors.\n\nname_arronds &lt;- unique(data_clean_all$NOM_COM)\nname_arronds &lt;- name_arronds[!is.na(name_arronds)]\nece_arrond &lt;- map_dbl(\n  .x = name_arronds,\n  .f = ~compute_ece_arrondissement(\n    arrond = .x, \n    k = 5, \n    obs_name = \"pm2\", \n    pred_name = \"pm2_estimated\",\n    data = data_clean_all\n  )\n)\n\nLet us put the values in a tibble:\n\nece_arrond_tb &lt;- tibble(\n  arrondissement = name_arronds,\n  ece = ece_arrond\n) |&gt; \n  mutate(\n    arrondissement = factor(\n      arrondissement, \n      levels = str_c(\n        \"Paris \", 1:20, \"e\", c(\"r\", rep(\"\", 19)), \" Arrondissement\")\n    )\n  ) |&gt; \n  arrange(arrondissement)\n\nWe can save this for later use (in Section 4.5 in Chapter 4).\n\nsave(ece_arrond_tb, file = \"../data/ece_arrond_tb.rda\")\n\nThe values are reported in Table 3.1.\n\n\nDisplay the codes used to create the Table\nknitr::kable(\n  ece_arrond_tb,\n    booktabs = TRUE, digits = 3,\n    col.names = c(\"Arrondissement\", \"ECE\")\n  )\n\n\n\n\nTable 3.1: Expected Calibration Error per Arrondissement.\n\n\nArrondissement\nECE\n\n\n\n\nParis 1er Arrondissement\n0.188\n\n\nParis 2e Arrondissement\n0.097\n\n\nParis 3e Arrondissement\n0.126\n\n\nParis 4e Arrondissement\n0.116\n\n\nParis 5e Arrondissement\n0.131\n\n\nParis 6e Arrondissement\n0.126\n\n\nParis 7e Arrondissement\n0.170\n\n\nParis 8e Arrondissement\n0.157\n\n\nParis 9e Arrondissement\n0.120\n\n\nParis 10e Arrondissement\n0.132\n\n\nParis 11e Arrondissement\n0.136\n\n\nParis 12e Arrondissement\n0.102\n\n\nParis 13e Arrondissement\n0.052\n\n\nParis 14e Arrondissement\n0.089\n\n\nParis 15e Arrondissement\n0.072\n\n\nParis 16e Arrondissement\n0.055\n\n\nParis 17e Arrondissement\n0.096\n\n\nParis 18e Arrondissement\n0.029\n\n\nParis 19e Arrondissement\n0.078\n\n\nParis 20e Arrondissement\n0.088"
  },
  {
    "objectID": "model-calibration.html#sec-calib-mont-champ",
    "href": "model-calibration.html#sec-calib-mont-champ",
    "title": "3  Model Calibration",
    "section": "3.7 Focus on Montmartre and Champs-de-Mars",
    "text": "3.7 Focus on Montmartre and Champs-de-Mars\nLet us now focus on two IRIS: Montmartre and Champs-de-Mars. We will compute the ECE for these two IRIS, varying the number of neighbors used to aggregate data.\nIn Section 2.3 from Chapter 2, we computed the minimum distance from one iris to another, considering distances up to 30. We will also need this information.\n\nneighbours_all &lt;- read_csv('../data/neighbours/all_neighbours_paris.csv')\n\nLet us define a function, compute_ece_iris_neighbors() that computes the ECE for a given IRIS code, using the neighborhood up to a certain degree.\n\n#' Computes ECE for a single IRIS, depending on the neighbors distance\n#' \n#' @param iris_code IRIS identifier\n#' @param k number of bins\n#' @param num_neigh distance of neighbors to include\n#' @param obs_name name of the variable with observed values in the data\n#' @param pred_name name of the variable with predicted values in the data\n#' @param data dataset to use\ncompute_ece_iris_neighbors &lt;- function(iris_code, \n                                       k, \n                                       num_neigh,\n                                       obs_name = \"pm2\", \n                                       pred_name = \"pm2_estimated\",\n                                       data,\n                                       neighbors_dist) {\n  \n  want_iris &lt;- \n    neighbors_dist |&gt; \n    filter(from_iris == iris_code) |&gt; \n    filter(distance &lt;= num_neigh) |&gt; \n    pull(to_iris) |&gt; \n    unique()\n  \n  data_current &lt;- data |&gt; filter(CODE_IRIS %in% !!want_iris)\n  obs &lt;- data_current |&gt; pull(!!obs_name)\n  pred &lt;- data_current |&gt; pull(!!pred_name)\n  ece &lt;- calculate_ece(\n    obs = obs, \n    pred = pred,\n    k = k\n  )\n  ece\n}\n\nLet us apply this function to Montmartre and Champs-de-Mars, and let us make the maximum distance of neighbors to include vary from 1 to 9 in steps of 1. For Montmartre:\n\nece_montmartre_neigh &lt;- map_dbl(\n  .x = 1:9,\n  .f = ~compute_ece_iris_neighbors(\n    iris_code = \"751093503\", \n    k = 5,\n    num_neigh = .x, \n    obs_name = \"pm2\",\n    pred_name = \"pm2_estimated\", \n    data = data_clean_all, \n    neighbors_dist = neighbours_all\n  )\n)\n\nAnd for Champs-de-Mars:\n\nece_champ_neigh &lt;- map_dbl(\n  .x = 1:9,\n  .f = ~compute_ece_iris_neighbors(\n    iris_code = \"751072812\", \n    k = 5,\n    num_neigh = .x, \n    obs_name = \"pm2\",\n    pred_name = \"pm2_estimated\", \n    data = data_clean_all, \n    neighbors_dist = neighbours_all\n  )\n)\n\nLet us put the results into two tibbles:\n\nece_neighbours_montmartre &lt;- tibble(\n  neighbours = 1:9,\n  ece = ece_montmartre_neigh,\n  iris_name = \"Montmartre\"\n) \n\nece_neighbours_mars &lt;- tibble(\n  neighbours = 1:9,\n  ece = ece_champ_neigh,\n  iris_name = \"Champs-de-Mars\"\n)\n\nAnd save the results for later use (in Section 4.4 in Chapter 4).\n\nsave(ece_neighbours_montmartre, file = \"../data/ece_neighbours_montmartre.rda\")\nsave(ece_neighbours_mars, file = \"../data/ece_neighbours_mars.rda\")\n\nThe values are reported in Table 3.2.\n\n\nDisplay the codes used to create the Table\nece_neighbours_montmartre |&gt; \n  bind_rows(ece_neighbours_mars) |&gt; \n  pivot_wider(names_from = iris_name, values_from = ece) |&gt; \n  rename(Neighbors = neighbours) |&gt; \n  knitr::kable(\n    booktabs = TRUE, digit = 3\n  )\n\n\n\n\nTable 3.2: Expected Calibration Error in Montmartre and Champs-de-Mars depending on the number of neighbors considered to compute the metric.\n\n\nNeighbors\nMontmartre\nChamps-de-Mars\n\n\n\n\n1\n0.190\n0.159\n\n\n2\n0.132\n0.077\n\n\n3\n0.129\n0.047\n\n\n4\n0.065\n0.038\n\n\n5\n0.026\n0.029\n\n\n6\n0.029\n0.033\n\n\n7\n0.025\n0.026\n\n\n8\n0.035\n0.029\n\n\n9\n0.044\n0.025\n\n\n\n\n\n\n\n\n\n\nKrüger, Fabian, and Johanna F. Ziegel. 2020. “Generic Conditions for Forecast Dominance.” Journal of Business & Economic Statistics 39 (2021): 972–83.\n\n\nNaeini, Mahdi Pakdaman, Gregory Cooper, and Milos Hauskrecht. 2015. “Obtaining Well Calibrated Probabilities Using Bayesian Binning.” In Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 29. 1.\n\n\nWidmann, David, Fredrik Lindsten, and Dave Zachariah. 2019. “Calibration Tests in Multi-Class Classification: A Unifying Framework.” Advances in Neural Information Processing Systems 32."
  },
  {
    "objectID": "fairness.html#background",
    "href": "fairness.html#background",
    "title": "4  Fairness",
    "section": "4.1 Background",
    "text": "4.1 Background\nRecall that our objective is to predict outcomes (prices) within an ordered set \\(\\mathcal{Y} := [K] = \\{1, \\ldots, K\\}\\). We thus face a multi-class classification framework. We use definitions of fairness that are suitable in this framework (see, e.g., Alghamdi et al. (2022) or Denis et al. (2021)).\n\n4.1.1 Demographic Parity (DP)\nLet \\(\\hat{Y}\\) be the output of the predictive model \\(h\\in\\mathcal{H}\\) defined on \\(\\mathcal{X}\\). From the algorithmic fairness literature, the (empirical) unfairness under DP is defined as follows:\n\n\n\n\n\n\nFairness under Demographic Parity\n\n\n\nThe unfairness under DP of a classifier \\(h\\) is quantified by \\[\n\\mathcal{U}_{DP}(h) := \\max_{a\\in\\mathcal{A}, k\\in[K]} \\left|\\, \\hat{\\mathbb{P}}(\\hat{Y} = k | ±\\, A = a) - \\hat{\\mathbb{P}}(\\hat{Y}  = k)\\, \\right|\\enspace,\n\\]\nwhere (A ) with ( := [M] = {1, , M}) is a discrete group representing specific geographic locations, which constitutes our sentitive attribute.\nA model \\(h\\) is called (empirically) exactly fair under DP i.f.f. \\(\\mathcal{U}_{DP}(h) = 0\\).\n\n\nWhen the label \\(Y\\) is assumed to be unbiased, there emerges a preference for a more nuanced measure of unfairness. Specifically, DP may hinder the realization of an ideal prediction scenario, such as granting loans precisely to those who are unlikely to default.\n\n\n4.1.2 Equalized Odds (EO)\nWe assume knowledge of the true and unbiased label \\(Y\\). The fairness measure under EO is defined as follows:\n\n\n\n\n\n\nFairness under Equalized Odds\n\n\n\nThe unfairness under EO of a classifier \\(h\\) is quantified by \\[\n\\mathcal{U}_{EO}(h) := \\max_{a\\in\\mathcal{A}, k, k'\\in[K]} \\left|\\,\\hat{\\mathbb{P}}(\\hat{Y} = k |Y \\, = k', \\,A = a) - \\hat{\\mathbb{P}}(\\hat{Y} = k | \\,Y = k'\\,)\\right|\\enspace.\n\\tag{4.1}\\]\nA model \\(h\\) is called (empirically) fair under EO i.f.f. \\(\\mathcal{U}_{EO}(h) = 0\\).\n\n\nIn R, we define the eo_measure() function to compute component of the Equalized Odds formula, for a given protected group \\(a\\).\n\n#' Calculate Equalized Odds Metrics\n#' \n#' @param obs_name name of the variable with observed values in the data\n#' @param pred_name name of the variable with predicted values in the data\n#' @param quantile_cutoffs quantile cutoffs to use to partition observed and \n#'   predicted values\n#' @param group_1 CODE_IRIS belonging to the group of interest ($a$)\n#' @param baseline_data data with all the observations\n#' \n#' @returns a tibble where each row corresponds to a combination of levels of\n#'   the predicted value ($k$, column `quant_predicted`) and the observed \n#'   value ($k'$, column `quant_observed`). For each row, the column \n#'   `value_diff` gives $\\hat{P}(\\hat{Y} = k | Y = k', A=a) -$ \n#'   $\\hat{P}(\\hat{Y} = k | Y = k')$ ()\neo_measure &lt;- function(obs_name = \"pm2\",\n                       pred_name = \"pm2_estimated\",\n                       quantile_cutoffs,\n                       group_1,\n                       baseline_data){\n  \n  # Assign each bin (based on quantile_cutoffs) to the observed and to the\n  # predicted values\n  data &lt;- \n    baseline_data |&gt; \n    mutate(\n      cut_observed = cut(\n        !!sym(obs_name), \n        quantile_cutoffs, \n        c(1:(length(quantile_cutoffs) - 1))\n      )\n    ) |&gt; \n    mutate(\n      cut_predictions = cut(\n        !!sym(pred_name), \n        quantile_cutoffs, \n        c(1:(length(quantile_cutoffs) - 1))\n      )\n    )\n  \n  retainer_1 &lt;- c()\n  retainer_2 &lt;- c()\n  value_retainer &lt;- c()\n  # Looping over classes (k)\n  for (level_1 in c(1:(length(quantile_cutoffs) - 1))) {\n    # Looping over classes (k')\n    for (level_2 in c(1:(length(quantile_cutoffs) - 1))) {\n      \n      # Identify whether Y==k & \\hat{Y} == k'\n      bucket_tmp &lt;- \n        data |&gt; \n        select(\n          CODE_IRIS, !!obs_name, !!pred_name, cut_observed, cut_predictions\n        ) |&gt; \n        mutate(\n          in_bucket = if_else(\n            cut_observed == level_1 & cut_predictions == level_2, 1, 0)\n        )\n      \n      # \\hat{P}(\\hat{Y} = k | Y = k')\n      p_average &lt;- \n        bucket_tmp |&gt; \n        pull(in_bucket)|&gt; \n        mean(na.rm = T)\n      \n      # \\hat{P}(\\hat{Y} = k | Y = k', A=a)\n      p_special &lt;- \n        bucket_tmp|&gt; \n        filter(CODE_IRIS %in% group_1) |&gt; \n        pull(in_bucket) |&gt; \n        mean(na.rm = T)\n      \n      # Store this (we need to find the max among those at the end of the loop)\n      value_tmp &lt;- abs(p_special - p_average)\n      \n      value_retainer &lt;- c(value_retainer, value_tmp)\n      retainer_1 &lt;- c(retainer_1, level_1)\n      retainer_2 &lt;- c(retainer_2, level_2)\n    }\n  }\n  \n tibble(\n    value_diff = value_retainer, \n    quant_observed = retainer_1, \n    quant_estimated = retainer_2\n  )\n}"
  },
  {
    "objectID": "fairness.html#sec-load-data-fairness",
    "href": "fairness.html#sec-load-data-fairness",
    "title": "4  Fairness",
    "section": "4.2 Load Data",
    "text": "4.2 Load Data\nLet us load the real estate data that were cleaned in Section 1.5 in Chapter 1.\n\nload(\"../data/data_clean_all.rda\")\n\nLet us also load the Parisian map saved in Section 1.3 from Chapter 1.\n\nload(\"../data/shapes.rda\")\n\nIn Section 2.3 from Chapter 2, we computed the minimum distance from one iris to another, considering distances up to 30. We will also need this informations.\n\nneighbours_all &lt;- read_csv('../data/neighbours/all_neighbours_paris.csv')"
  },
  {
    "objectID": "fairness.html#equalized-odds",
    "href": "fairness.html#equalized-odds",
    "title": "4  Fairness",
    "section": "4.3 Equalized Odds",
    "text": "4.3 Equalized Odds\nLet us compute the Equalized Odds, using the eo_measure() function. We will consider the predicted prices as well as some randomly drawn values. In each case, we will compute the Equalized Odds.\n\n4.3.1 EO with Predicted Prices\nWe need to define a partitioning of the data. We consider the quantiles of the observed price as the cutoffs. We will make the number of neighbors used to spatially smooth data vary. But before doing so, we would like to spend some time with a small example.\n\nlimits_quants &lt;- \n  data_clean_all |&gt; \n  pull(pm2) |&gt; \n  quantile(seq(0,1,0.2)) |&gt; \n  unname()\nlimits_quants\n\n[1]    11.97183  8893.48074  9941.87479 10829.77717 12122.78446 19994.28735\n\n\nWe want to examine the variation of the EO depending on the aggregation considered. Let us consider the immediate neighbors to begin with.\n\nnum_neigh &lt;- 1\n\nWe will focus on two IRIS: Montmartre and Champs-de-Mars (see Figure 2.4 from Chapter 2 to locate those two IRIS on the Parisian map). We extract the IRIS codes of those two IRIS.\n\nwant_montmartre &lt;- \n  neighbours_all |&gt; \n    filter(from_iris == '751093503') |&gt; \n    filter(distance &lt;= num_neigh) |&gt; \n    pull(to_iris)\nwant_mars &lt;- \n  neighbours_all |&gt; \n    filter(from_iris == '751072812') |&gt; \n    filter(distance &lt;= num_neigh) |&gt; \n    pull(to_iris)\n\nThen, we can compute the components of the EO formula, for each combination of \\(k\\) and \\(k'\\) (see Equation 4.1).\n\nre_mont &lt;- eo_measure(\n  obs_name = \"pm2\",\n  pred_name = \"pm2_estimated\",\n  quantile_cutoffs = limits_quants, \n  group_1 = want_montmartre, \n  baseline_data = data_clean_all\n)\nre_mont\n\n# A tibble: 25 × 3\n   value_diff quant_observed quant_estimated\n        &lt;dbl&gt;          &lt;int&gt;           &lt;int&gt;\n 1   0.119                 1               1\n 2   0.0462                1               2\n 3   0.0214                1               3\n 4   0.000272              1               4\n 5   0.00242               1               5\n 6   0.0438                2               1\n 7   0.0455                2               2\n 8   0.0115                2               3\n 9   0.0327                2               4\n10   0.00260               2               5\n# ℹ 15 more rows\n\n\nLet us extract, among these elements, the maximum value:\n\neo_mont &lt;- \n  re_mont |&gt; \n   arrange(desc(value_diff)) |&gt; \n    head(1) |&gt; \n    pull(value_diff)\n\nWe do the same for Champs de Mars:\n\nre_mars &lt;- eo_measure(\n  obs_name = \"pm2\",\n  pred_name = \"pm2_estimated\",\n  quantile_cutoffs = limits_quants, \n  group_1 = want_mars, \n  baseline_data = data_clean_all\n)\n\neo_mars &lt;- \n  re_mars |&gt; \n   arrange(desc(value_diff)) |&gt; \n    head(1) |&gt; \n    pull(value_diff)\neo_mars\n\n[1] 0.7387795\n\n\nNow, let us encompass the previous code inside a loop to consider different spatial aggregation levels.\n\nall_eo_mars &lt;- c()\nall_eo_mont &lt;- c()\nfor (num_neigh in c(1:9)) {\n  # Montmartre----\n  want_montmartre &lt;- \n    neighbours_all |&gt; \n    filter(from_iris == '751093503') |&gt; \n    filter(distance &lt;= num_neigh) |&gt; \n    pull(to_iris)\n  \n  re_mont &lt;- eo_measure(\n    obs_name = \"pm2\",\n    pred_name = \"pm2_estimated\",\n    quantile_cutoffs = limits_quants, \n    group_1 = want_montmartre, \n    baseline_data = data_clean_all\n  )\n  \n  eo_mont_value_tmp &lt;- \n    re_mont |&gt; \n    arrange(desc(value_diff)) |&gt; \n    head(1) |&gt; \n    pull(value_diff)\n  \n  all_eo_mont &lt;- c(all_eo_mont, eo_mont_value_tmp)\n\n  # Champs-de-Mars----\n  want_mars &lt;- \n    neighbours_all %&gt;% \n    filter(from_iris == '751072812') |&gt; \n    filter(distance &lt;= num_neigh) |&gt; \n    pull(to_iris)\n  \n  re_mars &lt;- eo_measure(\n    obs_name = \"pm2\",\n    pred_name = \"pm2_estimated\",\n    quantile_cutoffs = limits_quants, \n    group_1 = want_mars, \n    baseline_data = data_clean_all\n  )\n  \n  eo_mars_value_tmp &lt;- \n    re_mars |&gt; \n    arrange(desc(value_diff)) |&gt; \n    head(1) |&gt; \n    pull(value_diff)\n  \n  all_eo_mars &lt;- c(all_eo_mars, eo_mars_value_tmp)\n}\n\nWe can then store the EO computed for Montmartre and for Champs-de-Mars, depending on the number of neighbors considered.\n\ndata_eo_res &lt;- tibble(\n  neighbours = c(1:9),\n  all_mars = all_eo_mars, \n  all_nine = all_eo_mont, \n)\ndata_eo_res\n\n# A tibble: 9 × 3\n  neighbours all_mars all_nine\n       &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1          1   0.739    0.119 \n2          2   0.449    0.107 \n3          3   0.290    0.106 \n4          4   0.184    0.0750\n5          5   0.178    0.0421\n6          6   0.165    0.0318\n7          7   0.158    0.0239\n8          8   0.133    0.0202\n9          9   0.0911   0.0279\n\n\n\n\n4.3.2 EO with Random Values\nLet us now turn to the evaluation of EO where we no longer use the predicted prices, but rather draw random values, in a similar fashion to what was done in Section 3.5.2 from Chapter 3.\nWe define a function, eo_measure_random(){R} that will compute the EO based on random values for the predicted prices. This function works as follows:\n\nSimulation of observed prices:\n\n\nwe draw values from a Uniform distribution, where the bounds are the price range from the estimated prices\n\n\n\n\n\n#' Calculate Equalized Odds Metrics using randomly drawn predicted values\n#' \n#' @param obs_name name of the variable with observed values in the data\n#' @param pred_name name of the variable with predicted values in the data\n#' @param quantile_cutoffs quantile cutoffs to use to partition observed and \n#'   predicted values\n#' @param baseline_data data with all the observations\n#' \n#' @returns a list with two elements:\n#'  - `data_random` the data set with randomly drawn values for the prediction\n#'  - `metrics`: #' a tibble where each row corresponds to a combination of levels of\n#'   the predicted value ($k$, column `quant_predicted`) and the observed \n#'   value ($k'$, column `quant_observed`). For each row, the column \n#'   `value_diff` gives $\\hat{P}(\\hat{Y} = k | Y = k', A=a) -$ \n#'   $\\hat{P}(\\hat{Y} = k | Y = k')$ ()\neo_measure_random &lt;- function(obs_name = \"pm2\",\n                              pred_name = \"pm2_estimated\",\n                              quantile_cutoffs,\n                              baseline_data) {\n  \n  # Simulate estimated prices----\n  \n  # bounds for the Uniform\n  range_prices &lt;- \n    baseline_data |&gt; \n    pull(!!pred_name) |&gt; \n    range()\n  # No values to draw\n  rand_obs &lt;- nrow(baseline_data)\n  # Draw values\n  random_prices &lt;- runif(rand_obs, range_prices[1], range_prices[2])\n  \n  # Replace observed values by random ones\n  data_random &lt;- baseline_data |&gt;  \n    mutate(!!pred_name := !!random_prices)\n  \n  # Assign each bin (based on quantile_cutoffs) to the observed and to the\n  # 'predicted' values (random data)\n  data_random &lt;- data_random |&gt; \n    mutate(\n      cut_observed = cut(\n        !!sym(obs_name),\n        quantile_cutoffs, \n        c(1:(length(quantile_cutoffs) - 1))\n      )\n    )|&gt; \n    mutate(\n      cut_predictions = cut(\n        !!sym(pred_name),\n        quantile_cutoffs, \n        c(1:(length(quantile_cutoffs) - 1))\n      )\n    )\n  \n  # Assign each bin (based on quantile_cutoffs) to the observed and to the\n  # predicted values (baseline data)\n  data &lt;- baseline_data |&gt; \n    mutate(\n      cut_observed = cut(\n        !!sym(obs_name),\n        quantile_cutoffs, \n        c(1:(length(quantile_cutoffs) - 1))\n      )\n    )|&gt; \n    mutate(\n      cut_predictions = cut(\n        !!sym(pred_name),\n        quantile_cutoffs, \n        c(1:(length(quantile_cutoffs) - 1))\n      )\n    )\n  \n  \n  retainer_1 &lt;- c()\n  retainer_2 &lt;- c()\n  value_retainer &lt;- c()\n  # Looping over classes (k)\n  for (level_1 in c(1:(length(quantile_cutoffs) - 1))) {\n    # Looping over classes (k)\n    for (level_2 in c(1:(length(quantile_cutoffs) - 1))) {\n      \n      \n      ## Identify whether Y==k & \\hat{Y} == k' (baseline data)\n      bucket_tmp &lt;- \n        data |&gt; \n        select(\n          CODE_IRIS, !!obs_name, !!pred_name, cut_observed, cut_predictions\n        ) |&gt; \n        mutate(\n          in_bucket = if_else(\n            cut_observed == level_1 & cut_predictions == level_2, 1, 0)\n        )\n      \n      # (random data)\n      bucket_random_tmp &lt;- \n        data_random |&gt; \n        select(\n          CODE_IRIS, !!obs_name, !!pred_name, cut_observed, cut_predictions\n        ) |&gt; \n        mutate(\n          in_bucket = if_else(\n            cut_observed == level_1 & cut_predictions == level_2, 1, 0)\n        )\n      \n      ## \\hat{P}(\\hat{Y} = k | Y = k') (on baseline data)\n      p_average &lt;- bucket_tmp |&gt; \n        pull(in_bucket) |&gt; \n        mean(na.rm = T)\n      \n      ## \\hat{P}(\\hat{Y} = k | Y = k', A=a) (on random data)\n      p_special &lt;-\n        bucket_random_tmp |&gt; \n        pull(in_bucket) |&gt; \n        mean(na.rm = T)\n      \n      # Store this (we need to find the max among those at the end of the loop)\n      value_tmp &lt;- abs(p_special - p_average)\n      \n      value_retainer &lt;- c(value_retainer, value_tmp)\n      retainer_1 &lt;- c(retainer_1, level_1)\n      retainer_2 &lt;- c(retainer_2, level_2)\n    }\n  }\n  \n  list(\n    data_random = data_random,\n    metrics = tibble(\n      value_diff = value_retainer, \n      quant_observed = retainer_1, \n      quant_estimated = retainer_2\n    )\n  )\n}\n\nWe compute the components of the EO formula:\n\nre &lt;- eo_measure_random(\n  obs_name = \"pm2\",\n  pred_name = \"pm2_estimated\",\n  quantile_cutoffs = limits_quants, \n  baseline_data = data_clean_all\n)\ndata_random &lt;- re$data_random\nre$metrics\n\n# A tibble: 25 × 3\n   value_diff quant_observed quant_estimated\n        &lt;dbl&gt;          &lt;int&gt;           &lt;int&gt;\n 1    0.0500               1               1\n 2    0.0412               1               2\n 3    0.00860              1               3\n 4    0.00707              1               4\n 5    0.0927               1               5\n 6    0.0205               2               1\n 7    0.0741               2               2\n 8    0.0373               2               3\n 9    0.00385              2               4\n10    0.0948               2               5\n# ℹ 15 more rows\n\n\nThen, among these elements, we extract the maximum value:\n\nrandom_eo &lt;- \n  re$metrics |&gt; \n  arrange(desc(value_diff)) |&gt; \n  head(1) |&gt; \n  pull(value_diff)\nrandom_eo\n\n[1] 0.09482473"
  },
  {
    "objectID": "fairness.html#sec-fairness-viz-iris",
    "href": "fairness.html#sec-fairness-viz-iris",
    "title": "4  Fairness",
    "section": "4.4 Visualization of the Results",
    "text": "4.4 Visualization of the Results\nLet us now visualize how the EO metrics expands as the level of spatial aggregation increases, starting from the two IRIS regions corresponding to Champs-de-Mars and Montmartre.\nLet us isolate each level of neighbors for Champs-de-Mars and Montmartre. The following loop will create objects named full_champ_1 (immediate neighbors), full_champ_2 (neighbors of neighbors), etc. up to full_champ_8\n\nfor (num_neigh in 1:8) {\n  full_champs_current &lt;- \n    shapes_paris |&gt; \n    left_join(\n      neighbours_all |&gt; \n        filter(from_iris == '751072812') |&gt; \n        filter(distance == !!num_neigh) |&gt; \n        mutate(is_neigh = 'yes') |&gt; \n        mutate(CODE_IRIS = as.character(to_iris)) |&gt; \n        select(CODE_IRIS, is_neigh),\n      by = \"CODE_IRIS\"\n    ) |&gt; \n    mutate(is_neigh = if_else(is.na(is_neigh), 'no', 'yes')) |&gt; \n    group_by(is_neigh) |&gt; \n    summarise(comb_lev_1 = st_union(geometry)) |&gt; \n    filter(is_neigh == 'yes')\n  \n  assign(str_c(\"full_champ_\", num_neigh), value = full_champs_current)\n}\n\nLet us do something similar for Montmartre.\n\nfor (num_neigh in 1:8) {\n  full_mont_current &lt;- \n    shapes_paris |&gt; \n    left_join(\n      neighbours_all |&gt; \n        filter(from_iris == '751093503') |&gt; \n        filter(distance == !!num_neigh) |&gt; \n        mutate(is_neigh = 'yes') |&gt; \n        mutate(CODE_IRIS = as.character(to_iris)) |&gt; \n        select(CODE_IRIS, is_neigh),\n      by = \"CODE_IRIS\"\n    ) |&gt; \n    mutate(is_neigh = if_else(is.na(is_neigh), 'no', 'yes')) |&gt; \n    group_by(is_neigh) |&gt; \n    summarise(comb_lev_1 = st_union(geometry)) |&gt; \n    filter(is_neigh == 'yes')\n  \n  assign(str_c(\"full_mont_\", num_neigh), value = full_mont_current)\n}\n\nWe will use the following colors to identify the distance for the neighbors:\n\ncolors_want &lt;- terrain.colors(9)\n\n\nChamps-de-MarsMontmartre\n\n\n\n\nDisplay the codes used to create the Figure.\nmap_champ &lt;- \n  shapes_paris |&gt; \n  mutate(centroid = st_centroid(geometry)) |&gt; \n  ggplot() +\n  geom_sf() + \n  geom_sf(data = full_champ_8, fill = colors_want[8], color = 'black') + \n  geom_sf(data = full_champ_7, fill = colors_want[7], color = 'black') + \n  geom_sf(data = full_champ_6, fill = colors_want[6], color = 'black') + \n  geom_sf(data = full_champ_5, fill = colors_want[5], color = 'black') + \n  geom_sf(data = full_champ_4, fill = colors_want[4], color = 'black') + \n  geom_sf(data = full_champ_3, fill = colors_want[3], color = 'black') + \n  geom_sf(data = full_champ_2, fill = colors_want[2], color = 'black') + \n  geom_sf(data = full_champ_1, fill = colors_want[1], color = 'black') + \n  geom_sf(data = shapes_seine, fill = col_seine) + \n  global_theme() +\n  theme(legend.position = 'bottom') + \n  labs(fill = 'EO measure')\n\nmap_champ\n\n\n\n\nFigure 4.1: Champs de Mars and its IRIS neighbors.\n\n\n\n\n\n\n\n\n\nDisplay the codes used to create the Figure.\nmap_mont &lt;- \n  shapes_paris |&gt; \n  mutate(centroid = st_centroid(geometry)) |&gt; \n  ggplot() +\n  geom_sf() + \n  geom_sf(data = full_mont_8, fill = colors_want[8], color = 'black') + \n  geom_sf(data = full_mont_7, fill = colors_want[7], color = 'black') + \n  geom_sf(data = full_mont_6, fill = colors_want[6], color = 'black') + \n  geom_sf(data = full_mont_5, fill = colors_want[5], color = 'black') + \n  geom_sf(data = full_mont_4, fill = colors_want[4], color = 'black') + \n  geom_sf(data = full_mont_3, fill = colors_want[3], color = 'black') + \n  geom_sf(data = full_mont_2, fill = colors_want[2], color = 'black') + \n  geom_sf(data = full_mont_1, fill = colors_want[1], color = 'black') + \n  geom_sf(data = shapes_seine, fill = col_seine) + \n  global_theme() +\n  theme(legend.position = 'bottom') + \n  labs(fill = 'EO measure')\n\nmap_mont\n\n\n\n\nFigure 4.2: Montmartre and its IRIS neighbors.\n\n\n\n\n\n\n\n\nNow, let us plot the EO measure as a function of the neighbor level.\n\n\nDisplay the codes used to create the Figure.\nlineplot_eo &lt;- \n  data_eo_res |&gt; \n  select(\n    neighbors = neighbours, \n    `Champs de Mars` = all_mars, \n    Montmartre = all_nine\n  ) |&gt; \n  mutate(neighbors = as.character(neighbors)) |&gt; \n  pivot_longer(\n    cols = c(`Champs de Mars`:Montmartre),\n    names_to = \"IRIS region\", values_to = \"value\"\n  ) %&gt;% \n  ggplot(data = .) + \n  geom_hline(\n    mapping = aes(\n      yintercept = random_eo, \n      color = 'value for random estimation'\n    ), \n    lwd = 2, \n    lty = 'dashed'\n  ) +\n  geom_line(\n    mapping = aes(\n      x = neighbors,\n      y = value, \n      group = `IRIS region`\n    )\n  ) + \n  geom_point(\n    mapping = aes(\n      x = neighbors,\n      y = value,\n      fill = neighbors\n    ),\n    pch = 21,\n    size = 4\n  ) + \n  # Champs-de-Mars\n  geom_segment(\n    x = 2.5,\n    y = (data_eo_res$all_mars[2] + data_eo_res$all_mars[3]) / 2,\n    xend = 4,\n    yend = .5,\n    lty = 3\n  ) +\n  geom_segment(\n    x = 4,\n    y = .5,\n    xend = 8,\n    yend = .5,\n    lty = 3\n  ) +\n  annotate(\n    geom = \"text\", x = 5, \n    y = .55, \n    label = \"Champs-de-Mars\",\n    hjust = 0\n  ) +\n  # Montmartre\n  geom_segment(\n    x = 3.5,\n    y = (data_eo_res$all_nine[3] + data_eo_res$all_nine[4]) / 2,\n    xend = 5,\n    yend = .3,\n    lty = 3\n  ) +\n  geom_segment(\n    x = 5,\n    y = .3,\n    xend = 8,\n    yend = .3,\n    lty = 3\n  ) +\n  annotate(\n    geom = \"text\", x = 5.75, \n    y = .35, \n    label = \"Montmartre\",\n    hjust = 0\n  ) +\n  scale_color_manual(values = c('lightgrey')) + \n  scale_fill_manual(values = colors_want) +\n  ylab(latex2exp::TeX(r'($U_{EO}(h)$)')) +\n  xlab('Neighbor level') + \n  global_theme() + \n  theme(\n    legend.position = 'bottom',\n    # legend.box=\"vertical\",\n    legend.text = element_text(family = font_main, size = 14)\n  ) + \n  guides(fill = guide_legend(override.aes = list(size=5), nrow=1)) + \n  guides(color = guide_legend(title='')) + \n  guides(linetype = guide_legend(title='')) + \n  ylim(c(0,0.739))\n\nlineplot_eo\n\n\n\n\nFigure 4.3: Equalized Odds Measure for Montmartre and Champ-de-Mars.\n\n\n\n\n\nLet us load the calibration errors computed for those two IRIS, in Section 3.7 in Chapter 3.\n\nload(\"../data/ece_neighbours_montmartre.rda\")\nload(\"../data/ece_neighbours_mars.rda\")\n\n\n\nDisplay the codes used to create the Figure.\nlineplot_ece &lt;- \n  ece_neighbours_montmartre |&gt; \n  bind_rows(ece_neighbours_mars) |&gt; \n  rename(`IRIS region` = iris_name) |&gt; \n  mutate(neighbors = as.character(neighbours), value = ece) |&gt; \n  ggplot() + \n  geom_hline(\n    mapping = aes(\n      yintercept = .0262022, # VALUE OBTAINED IN PREVIOUS CHAPTER\n      color = 'value for random estimation'\n    ), \n    lwd = 2, \n    lty = 'dashed'\n  ) +\n  geom_line(\n    mapping = aes(\n      x = neighbors,\n      y = ece, \n      group = `IRIS region`,\n    )\n  ) + \n  geom_point(\n    mapping = aes(\n      x = neighbors,\n      y = value,\n      fill = neighbors\n    ),\n    pch = 21,\n    size = 4\n  ) + \n  # Champs-de-Mars\n  geom_segment(\n    x = 1.5,\n    y = (ece_neighbours_mars$ece[1] + ece_neighbours_mars$ece[2]) / 2,\n    xend = 3,\n    yend = .5,\n    lty = 3\n  ) +\n  geom_segment(\n    x = 3,\n    y = .5,\n    xend = 7,\n    yend = .5,\n    lty = 3\n  ) +\n  annotate(\n    geom = \"text\", x = 4,\n    y = .55,\n    label = \"Champs-de-Mars\",\n    hjust = 0\n  ) +\n  # Montmartre\n  geom_segment(\n    x = 3.5,\n    y = (ece_neighbours_montmartre$ece[3] + ece_neighbours_montmartre$ece[4]) / 2,\n    xend = 5,\n    yend = .3,\n    lty = 3\n  ) +\n  geom_segment(\n    x = 5,\n    y = .3,\n    xend = 8,\n    yend = .3,\n    lty = 3\n  ) +\n  annotate(\n    geom = \"text\", x = 5.75, \n    y = .35, \n    label = \"Montmartre\",\n    hjust = 0\n  ) +\n  scale_color_manual(values = c('lightgrey')) + \n  scale_fill_manual(values = colors_want) +\n  ylab(latex2exp::TeX(r'($U_{ECE}(h)$)')) +\n  xlab('Neighbor level') + \n  global_theme() + \n  theme(\n    legend.position = 'bottom',\n    # legend.box=\"vertical\",\n    legend.text = element_text(family = font_main, size = 14)\n  ) + \n  guides(fill = guide_legend(override.aes = list(size = 5), nrow = 1)) + \n  guides(color = guide_legend(title = '')) + \n  guides(linetype = guide_legend(title='')) + \n  ylim(c(0,0.739))\n\nlineplot_ece\n\n\n\n\nFigure 4.4: Expected Calibration Error Measure for Montmartre and Champ-de-Mars."
  },
  {
    "objectID": "fairness.html#sec-fairness-eo-arrond",
    "href": "fairness.html#sec-fairness-eo-arrond",
    "title": "4  Fairness",
    "section": "4.5 EO on each Arrondissement",
    "text": "4.5 EO on each Arrondissement\nNow, we can compute the Equalized Odds metric on each arrondissement. For convenience, let us create a function, calculate_eo_arrond(), that computes EO on a single arrondissement.\n\n#' EO metric for an arrondissement\n#' \n#' @param arrond name of the arrondissement\n#' @param num_neigh distance of neighbors to include\n#' @param obs_name name of the variable with observed values in the data\n#' @param pred_name name of the variable with predicted values in the data\n#' @param data dataset to use\ncalculate_eo_arrond &lt;- function(arrond, \n                                num_neigh, \n                                obs_name = \"pm2\",\n                                pred_name = \"pm2_estimated\",\n                                data) {\n  \n  # Cutoff to partition data\n  limits_quants &lt;- \n    data |&gt; \n    pull(!!obs_name) |&gt; \n    quantile(seq(0,1,0.2)) |&gt; \n    unname()\n  \n  # Extract IRIS in the arrondissement\n  want_arrond &lt;- \n    data |&gt; \n    filter(NOM_COM %in% arrond) |&gt; \n    pull(CODE_IRIS) |&gt; \n    unique()\n  \n  re_arrond &lt;- eo_measure(\n    obs_name = obs_name,\n    pred_name = pred_name,\n    quantile_cutoffs = limits_quants, \n    group_1 = want_arrond, \n    baseline_data = data\n  )\n  \n  ece_arrond &lt;- \n    re_arrond |&gt; \n    arrange(desc(value_diff)) |&gt; \n    head(1) |&gt; \n    pull(value_diff)\n  \n  ece_arrond\n}\n\nAll that needs to be done is to loop over the names of the arrondissements.\n\nname_arronds &lt;- unique(data_clean_all$NOM_COM)\nname_arronds &lt;- name_arronds[!is.na(name_arronds)]\n\neo_arrond &lt;- map_dbl(\n  .x = name_arronds,\n  .f = ~calculate_eo_arrond(\n    arrond = .x, \n    num_neigh = 5, \n    obs_name = \"pm2\", \n    pred_name = \"pm2_estimated\", \n    data = data_clean_all\n  )\n)\n\nWe can put those values in a tibble:\n\neo_arrond_tb &lt;- tibble(\n  arrondissement = name_arronds,\n  ece = eo_arrond\n) |&gt; \n  mutate(\n    arrondissement = factor(\n      arrondissement, \n      levels = str_c(\n        \"Paris \", 1:20, \"e\", c(\"r\", rep(\"\", 19)), \" Arrondissement\")\n    )\n  ) |&gt; \n  arrange(arrondissement)\n\nThe values can be saved:\n\nsave(eo_arrond_tb, file = \"../data/eo_arrond_tb.rda\")\n\nFor comparison with the Expected Calibration Error, let us load the results obtained in Section 3.6 in Chapter 3.\n\nload(\"../data/ece_arrond_tb.rda\")\n\nThe values are reported in Table 4.1.\n\n\nDisplay the codes used to create the Table\nece_arrond_tb |&gt; \n  left_join(\n    eo_arrond_tb, by = \"arrondissement\"\n  ) |&gt; \n  knitr::kable(\n    booktabs = TRUE, digits = 3,\n    col.names = c(\"Arrondissement\", \"ECE\", \"EO\")\n  )\n\n\n\n\nTable 4.1: Equalized Odds per Arrondissement.\n\n\nArrondissement\nECE\nEO\n\n\n\n\nParis 1er Arrondissement\n0.188\n0.301\n\n\nParis 2e Arrondissement\n0.097\n0.146\n\n\nParis 3e Arrondissement\n0.126\n0.319\n\n\nParis 4e Arrondissement\n0.116\n0.469\n\n\nParis 5e Arrondissement\n0.131\n0.280\n\n\nParis 6e Arrondissement\n0.126\n0.637\n\n\nParis 7e Arrondissement\n0.170\n0.625\n\n\nParis 8e Arrondissement\n0.157\n0.128\n\n\nParis 9e Arrondissement\n0.120\n0.117\n\n\nParis 10e Arrondissement\n0.132\n0.114\n\n\nParis 11e Arrondissement\n0.136\n0.108\n\n\nParis 12e Arrondissement\n0.102\n0.115\n\n\nParis 13e Arrondissement\n0.052\n0.177\n\n\nParis 14e Arrondissement\n0.089\n0.070\n\n\nParis 15e Arrondissement\n0.072\n0.072\n\n\nParis 16e Arrondissement\n0.055\n0.107\n\n\nParis 17e Arrondissement\n0.096\n0.085\n\n\nParis 18e Arrondissement\n0.029\n0.145\n\n\nParis 19e Arrondissement\n0.078\n0.396\n\n\nParis 20e Arrondissement\n0.088\n0.242\n\n\n\n\n\n\n\n\n\n\nAlghamdi, Wael, Hsiang Hsu, Haewon Jeong, Hao Wang, Peter Michalak, Shahab Asoodeh, and Flavio Calmon. 2022. “Beyond Adult and COMPAS: Fair Multi-Class Prediction via Information Projection.” Advances in Neural Information Processing Systems 35: 38747–60.\n\n\nCalders, T., F. Kamiran, and M. Pechenizkiy. 2009. “Building Classifiers with Independency Constraints.” In IEEE International Conference on Data Mining.\n\n\nDenis, Christophe, Romuald Elie, Mohamed Hebiri, and François Hu. 2021. “Fairness Guarantee in Multi-Class Classification.” arXiv Preprint arXiv:2109.13642.\n\n\nHardt, M., E. Price, and N. Srebro. 2016. “Equality of Opportunity in Supervised Learning.” In Neural Information Processing Systems."
  },
  {
    "objectID": "mitigation.html#background",
    "href": "mitigation.html#background",
    "title": "5  Mitigation",
    "section": "5.1 Background",
    "text": "5.1 Background\nLet \\(\\hat{\\boldsymbol{p}}\\) represent the confidence scores obtained with a predictive model. Assuming the associated predictive model is fairness-aware, \\(\\hat{\\boldsymbol{p}}(\\boldsymbol{x}, a)\\) is therefore defined on \\(\\mathcal{X}\\times \\mathcal{A}\\), considering \\(\\mathcal{A}\\) a binary set.\nFollowing Denis et al. (2021), and readjusting the result for exact fairness, the new and fair scores \\(\\hat{\\boldsymbol{p}}^{(\\text{fair})} = (\\hat{p}^{(\\text{fair})}_1, \\ldots, \\hat{p}^{(\\text{fair})}_K)\\) are given by \\[\n\\hat{p}^{(\\text{fair})}_k(\\boldsymbol{x}, a) = \\hat{\\mathbb{P}}(A=a)\\cdot (\\hat{p}_{k}(\\boldsymbol{x}, a) - a \\cdot\\hat{\\lambda}_k)\\, ,\\;\\; \\mbox{for all }(\\boldsymbol{x}, a)\\in \\mathcal{X}\\times {\\mathcal{A}},\n\\]\nwhere this technique is dedicated to enforcing DP-fairness of the overall classification rule by shifting the estimated conditional probabilities in an optimal manner, as dictated by the calibrated parameters \\(\\hat{\\boldsymbol{\\lambda}} = (\\hat{\\lambda}_1, \\ldots, \\hat{\\lambda}_K)\\) (refer to Denis et al. (2021) for more details about its optimization).\nGiven these new scores, the associated optimal (DP-fair) predicted class is simply \\(\\argmax{k\\in [K]} p^{(\\text{fair})}_{k}\\).\nWe let the original price be \\(\\hat{Y} := h(X, A)\\), where \\(h\\) is an unknown predictive model defined on \\(\\mathcal{X}\\times\\mathcal{A}\\). We define the unfairness under DP as follows:\n\\[\n\\mathcal{U}_{DP}(h) := \\mathbb{P}(\\hat{Y} \\in [q_{i}, q_{i+1}] | A = a) - \\mathbb{P}(\\hat{Y} \\in [q_{i}, q_{i+1}] | A = b)\n\\]"
  },
  {
    "objectID": "mitigation.html#load-and-clean-data",
    "href": "mitigation.html#load-and-clean-data",
    "title": "5  Mitigation",
    "section": "5.2 Load and Clean Data",
    "text": "5.2 Load and Clean Data\nLet us load the real estate data first, those obtained in Section 4.2 in Chapter 4.\n\nimport pandas as pd\nimport numpy as np\n\n# Real-estate data\nparis_data = pd.read_csv(\"../data/data_clean_all.csv\", index_col=0)\n\nFor convenience, we create a column with the arrondissement number.\n\nparis_data['A'] = paris_data['NOM_COM'].str.extract(r'(\\d+)').astype(float).astype('Int64')\n\nLet us load the scores obtained in Section 3.2 in Chapter 3.\n\ndistances = pd.read_csv(\"../data/distances.csv\", index_col=0)\n\nLet us visualize the estimated prices as a function of the observed prices. The plot is visible in Figure 5.1.\n\n\nDisplay the codes used to create the Figure.\nimport matplotlib.pyplot as plt\nimport numpy as np\nrange_min = min(np.min(paris_data.pm2), np.min(paris_data.pm2_estimated))\nrange_max =  max(np.max(paris_data.pm2), np.max(paris_data.pm2_estimated))\nx = np.linspace(range_min, range_max, 1000)\nplt.plot(x, x, color=\"black\")\nplt.scatter(paris_data.pm2, paris_data.pm2_estimated, color=\"orange\", marker=\"+\", s=100)\nplt.xlabel(\"Observed square meter price\")\nplt.ylabel(\"Estimated square meter price\")\nplt.title(\"Well-calibrated?\")\n\nplt.show()\n\n\n\n\nFigure 5.1: Estimated price vs observed price (in Euros).\n\n\n\n\n\nLet us define variables with the observed values (y), and with predicted ones (y_pred).\n\ny = paris_data.pm2\ny_pred = paris_data.pm2_estimated\n\nLet us discretize the data. We will consider 5 bins, as in previous chapters.\n\nK = 5\n\n# Discretization of y\ncuts = np.quantile(np.array(y), q=np.linspace(0, 1, num = K+1))\n\n# apply on pred\ny_pred_categ = np.digitize(np.array(y_pred), bins=cuts, right=True)\ny_pred_categ[y_pred_categ == 0] = 1\n# apply on test\ny_categ = np.digitize(np.array(y), bins=cuts, right=True)\ny_categ[y_categ == 0] = 1\n\nThe discretized values are added to the dataset:\n\nparis_data[\"y_categ\"] = y_categ\nparis_data[\"y_pred_categ\"] = y_pred_categ\n\nLet us have a look at the accuracy:\n\nfrom sklearn.metrics import accuracy_score\n\n# Calculate accuracy score\naccuracy = accuracy_score(y_categ, y_pred_categ)\nprint(f'Accuracy: {accuracy:.2f}')\n\nAccuracy: 0.46\n\n\nNow, we can apply the softmax function to the predicted scores computed in Chapter 3.\n\nfrom scipy.special import softmax\n\nscores = softmax(-distances, axis=1)\nscores = pd.DataFrame(\n  scores, index = distances.index, columns=distances.columns)\n\n\nscores.reset_index()\n\n            id         1         2         3         4         5\n0      1624836  0.254176  0.480247  0.176673  0.064994  0.023910\n1      1613636  0.014758  0.040116  0.109047  0.296419  0.539660\n2      1612621  0.016790  0.045640  0.124061  0.337233  0.476276\n3      1605927  0.410082  0.379857  0.139741  0.051408  0.018912\n4      1605922  0.489094  0.328980  0.121025  0.044523  0.016379\n...        ...       ...       ...       ...       ...       ...\n11164   236654  0.043620  0.118571  0.322309  0.376861  0.138639\n11165   234198  0.030518  0.082957  0.225502  0.483246  0.177776\n11166   219084  0.046252  0.125727  0.341760  0.355485  0.130776\n11167   212675  0.045214  0.122904  0.334087  0.363918  0.133878\n11168   131156  0.025876  0.070337  0.191196  0.519726  0.192865\n\n[11169 rows x 6 columns]\n\n# index reset here to avoid showing real ID\n\nWe need to check if some observations which are present in only one of the two datasets. Let us first identify those, if any. The cases in the real-estate dataset but not in the scores dataset:\n\nfor ind in paris_data.index:\n    if ind not in scores.index:\n        print(ind)\n\nAnd the cases in the scores dataset but not in the real-estate one:\n\nfor ind in scores.index:\n    if ind not in paris_data.index:\n        print(ind)\n\nWe make sure the IRIS codes are stored as integers.\n\nparis_data[\"CODE_IRIS\"] = paris_data[\"CODE_IRIS\"].astype(int)\n\nLet us add the scores to the real-estate data.\n\nparis_data = pd.concat((paris_data, scores), axis=1)"
  },
  {
    "objectID": "mitigation.html#helper-functions",
    "href": "mitigation.html#helper-functions",
    "title": "5  Mitigation",
    "section": "5.3 Helper Functions",
    "text": "5.3 Helper Functions\nLet us define some helper functions.\n\nfrom scipy.optimize import minimize, LinearConstraint\n\ndef optposSLSQP(fun, n_classes):\n    ineq_cons = {\n        'type': 'ineq',\n        'fun' : lambda x: x}\n    lam0 = np.zeros(2*n_classes)\n    res = minimize(\n        fun,\n        lam0,\n        constraints=[ineq_cons],\n        options={'ftol': 1e-9, 'disp': False},\n        method='SLSQP')\n    lam = res.x\n    return lam[:n_classes], lam[n_classes:]\n\nWe define the prepare_fairness() function.\n\ndef prepare_fairness(y_probs, binary_sensitive):\n    ind0 = (binary_sensitive == -1)\n    ind1 = (binary_sensitive == 1)\n    p0 = np.mean(ind0)\n    p1 = np.mean(ind1)\n    ps = np.array([p0, p1])\n    y_prob_dict = dict()\n    y_prob_dict[0] = y_probs[ind0, :]\n    y_prob_dict[1] = y_probs[ind1, :]\n    return y_prob_dict, ps\n\nWe also define the fair_soft_max() function. This function readjusts scores.\n\ndef fair_soft_max(y_probs, binary_sensitive, K, c = 0.1, sigma = 10**(-5), epsilon_fair = 0.01):\n    \"\"\"\n    for the optimization technique we use \"slsqp\" optimization, but other methodologies can be used\n    \"\"\"\n    # computation of lambda (soft and hard)\n    y_prob_dict, ps = prepare_fairness(y_probs, binary_sensitive)\n    n_classes = K\n    \n    def bivar_fairness_soft(lam, n_classes = n_classes, c = c):\n        res = 0\n        lamb = lam[:n_classes]\n        beta = lam[n_classes:]\n        for s in [0,1]:\n            val = y_prob_dict[s]*ps[s] - (2*s-1)*(lamb - beta)\n            res += np.mean(np.sum(softmax(val/c, axis=1)*val, axis=1)) # Smooth arg max\n        res += epsilon_fair * np.sum(lamb + beta)\n        return res\n    \n    lam_soft, beta_soft = optposSLSQP(fun = bivar_fairness_soft, n_classes = n_classes)\n    \n    # inference with and without fairness\n    index_0 = np.where(binary_sensitive == -1)[0]\n    index_1 = np.where(binary_sensitive == 1)[0]\n    \n    eps = np.random.uniform(0, sigma, (y_probs.shape))\n    y_prob_fair_soft = np.zeros(y_probs.shape)\n    y_prob_fair_soft[index_0] = ps[0]*(y_probs[index_0]+eps[index_0]) - (-1)*(lam_soft-beta_soft)\n    y_prob_fair_soft[index_1] = ps[1]*(y_probs[index_1]+eps[index_1]) - 1*(lam_soft-beta_soft)\n    y_pred_fair_soft = np.argmax(y_prob_fair_soft, axis = 1) + 1\n    \n    return y_pred_fair_soft, y_prob_fair_soft, index_0, index_1\n\nWe also define the unfairness() function, which computes unfairness of two populations.\n\ndef unfairness(data1, data2):\n    \"\"\"\n    compute the unfairness of two populations\n    \"\"\"\n    K = int(np.max((np.max(data1), np.max(data2))))+1\n    nu_0 = np.zeros(K)\n    nu_1 = np.zeros(K)\n    \n    pos, counts = np.unique(data1, return_counts=True)\n    nu_0[pos.astype(int)] = counts/len(data1)\n    \n    pos, counts = np.unique(data2, return_counts=True)\n    nu_1[pos.astype(int)] = counts/len(data2)\n    \n    unfair_value = np.abs(nu_0 - nu_1).max()\n    return unfair_value"
  },
  {
    "objectID": "mitigation.html#sec-dp-fair-pred",
    "href": "mitigation.html#sec-dp-fair-pred",
    "title": "5  Mitigation",
    "section": "5.4 DP-Fair Predictions",
    "text": "5.4 DP-Fair Predictions\nWe will focus on some IRIS. For each IRIS,\n\ngroups = pd.read_csv(\"../data/some_locations.csv\")\ngroups.shape\n\n(11400, 22)\n\n\nWe can add these columns to the paris_data table.\n\ngroups = pd.merge(paris_data.reset_index()[[\"id\", \"1\", \"2\", \"3\", \"4\", \"5\", \"y_categ\", \"y_pred_categ\"]], groups, on='id', how='left')\ngroups.shape\n\n(11205, 29)\n\n\nThen, we will iterate on multiple areas of interest. For an area of interest, for example, mars_neigh_1, which corresponds to the IRIS in Champs-de-Mars and their immediate neighbors, we will compute the unfairness with respect to the rest of Paris. When we consider mars_neigh_2 as the area of interest, the values used to define Champs-de-Mars become the IRIS of Montmartre, its immediate neighbors, and the neighbors of its neighbors.\n\nnp.random.seed(42)\n\ny_probs = np.array(groups[[\"1\", \"2\", \"3\", \"4\", \"5\"]])\ny_test = groups.y_categ\n\ny_pred_fair_multi = []\ny_prob_fair_multi = []\n\ndebiased_groups = pd.DataFrame()\ndebiased_groups_scores = pd.DataFrame()\ndebiased_groups[[\"id\", \"CODE_IRIS\"]] = groups[[\"id\", \"CODE_IRIS\"]]\ndebiased_groups_scores[[\"id\", \"CODE_IRIS\"]] = groups[[\"id\", \"CODE_IRIS\"]]\n\n# Looping over different IRIS to consider as protected\nfor sensitive in ['mars_neigh_1', 'mont_neigh_1', 'mars_neigh_2',\n       'mont_neigh_2', 'mars_neigh_3', 'mont_neigh_3', 'mars_neigh_4',\n       'mont_neigh_4', 'mars_neigh_5', 'mont_neigh_5', 'mars_neigh_6',\n       'mont_neigh_6', 'mars_neigh_7', 'mont_neigh_7', 'mars_neigh_8',\n       'mont_neigh_8', 'mars_neigh_9', 'mont_neigh_9', 'in_12', 'in_20']:\n    \n    # Identity observations in the sensitive region\n    binary_sensitive = np.array(groups[sensitive])\n    \n    # Compute mitigated scores\n    y_pred_fair, y_prob_fair, index_0, index_1 = fair_soft_max(\n        y_probs,\n        binary_sensitive,\n        K,\n        c = 0.00001,\n        sigma = 0.00001,\n        epsilon_fair = 0.00001\n    )\n    y_pred_fair_multi.append(y_pred_fair)\n    y_prob_fair = softmax(y_prob_fair, axis=1)\n    y_prob_fair_multi.append(y_prob_fair)\n    \n    # Add the mitigated predicted class and scores in the table\n    debiased_groups[\"DP_class_for_\" + sensitive] = y_pred_fair\n    for k in range(K):\n        debiased_groups_scores[f\"DP_score_{k+1}_\" + sensitive] = y_prob_fair[:,k]\n    \n    print(\"For:\", sensitive)\n    unfair_measure = np.max((\n      unfairness(groups.y_pred_categ, groups.y_pred_categ[index_1]),\n      unfairness(groups.y_pred_categ, groups.y_pred_categ[index_0])))\n    \n    print(\"accuracy, unfairness (baseline): \", \n      accuracy_score(np.array(y_test), groups.y_pred_categ), \n      \"\\t\", unfair_measure)\n    \n    unfair_measure = np.max((\n      unfairness(y_pred_fair, y_pred_fair[index_1]), \n      unfairness(y_pred_fair, y_pred_fair[index_0])))\n    \n    print(\"accuracy, unfairness (mitigated): \",\n      accuracy_score(np.array(y_test), y_pred_fair), \"\\t\", unfair_measure)\n    print()\n\nFor: mars_neigh_1\naccuracy, unfairness (baseline):  0.46336456938866577    0.7597182380314911\naccuracy, unfairness (mitigated):  0.46247211066488175   0.08508956460763689\n\nFor: mont_neigh_1\naccuracy, unfairness (baseline):  0.46336456938866577    0.25024187995543623\naccuracy, unfairness (mitigated):  0.4607764390896921    0.004458156393048279\n\nFor: mars_neigh_2\naccuracy, unfairness (baseline):  0.46336456938866577    0.505304112830849\naccuracy, unfairness (mitigated):  0.4573850959393128    0.004075728632382203\n\nFor: mont_neigh_2\naccuracy, unfairness (baseline):  0.46336456938866577    0.16690582801924722\naccuracy, unfairness (mitigated):  0.4612226684515841    0.001079962982746513\n\nFor: mars_neigh_3\naccuracy, unfairness (baseline):  0.46336456938866577    0.3448310199863782\naccuracy, unfairness (mitigated):  0.4508701472556894    0.0043185607928791225\n\nFor: mont_neigh_3\naccuracy, unfairness (baseline):  0.46336456938866577    0.16506226927913678\naccuracy, unfairness (mitigated):  0.4596162427487729    0.0011662812867632155\n\nFor: mars_neigh_4\naccuracy, unfairness (baseline):  0.46336456938866577    0.22969094048826993\naccuracy, unfairness (mitigated):  0.4459616242748773    0.0018601958130773921\n\nFor: mont_neigh_4\naccuracy, unfairness (baseline):  0.46336456938866577    0.15938787831062814\naccuracy, unfairness (mitigated):  0.45872378402498887   0.000677218678636099\n\nFor: mars_neigh_5\naccuracy, unfairness (baseline):  0.46336456938866577    0.21681754116737956\naccuracy, unfairness (mitigated):  0.43525211958946897   0.0002924979048987142\n\nFor: mont_neigh_5\naccuracy, unfairness (baseline):  0.46336456938866577    0.1034107541276216\naccuracy, unfairness (mitigated):  0.45765283355644804   0.0004524765729584934\n\nFor: mars_neigh_6\naccuracy, unfairness (baseline):  0.46336456938866577    0.20261338744476481\naccuracy, unfairness (mitigated):  0.4145470771976796    8.529851586819293e-05\n\nFor: mont_neigh_6\naccuracy, unfairness (baseline):  0.46336456938866577    0.0756902615492353\naccuracy, unfairness (mitigated):  0.4556001784917448    0.00026638001965606506\n\nFor: mars_neigh_7\naccuracy, unfairness (baseline):  0.46336456938866577    0.1991717251102052\naccuracy, unfairness (mitigated):  0.39134315037929496   0.00010365307004758795\n\nFor: mont_neigh_7\naccuracy, unfairness (baseline):  0.46336456938866577    0.05664125648676674\naccuracy, unfairness (mitigated):  0.46166889781347614   7.520135124639005e-05\n\nFor: mars_neigh_8\naccuracy, unfairness (baseline):  0.46336456938866577    0.1739308640326483\naccuracy, unfairness (mitigated):  0.3741186970102633    0.00011519585530833654\n\nFor: mont_neigh_8\naccuracy, unfairness (baseline):  0.46336456938866577    0.04690105451043944\naccuracy, unfairness (mitigated):  0.4610441767068273    8.45487212005891e-05\n\nFor: mars_neigh_9\naccuracy, unfairness (baseline):  0.46336456938866577    0.13652482790770132\naccuracy, unfairness (mitigated):  0.3773315484158858    0.0002381354269786473\n\nFor: mont_neigh_9\naccuracy, unfairness (baseline):  0.46336456938866577    0.04315163871374325\naccuracy, unfairness (mitigated):  0.45194109772423025   1.3335022436167243e-05\n\nFor: in_12\naccuracy, unfairness (baseline):  0.46336456938866577    0.17290610370245987\naccuracy, unfairness (mitigated):  0.4574743418116912    0.0036579924032172084\n\nFor: in_20\naccuracy, unfairness (baseline):  0.46336456938866577    0.32289468675010846\naccuracy, unfairness (mitigated):  0.44810352521195895   0.002942617400448738\n\n\n&lt;string&gt;:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n&lt;string&gt;:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n\n\nAt the end of the loop, we have the DP-fair predicted class as well as the associated scores, for all the regions considered.\n\ndebiased_groups_scores.drop(\"id\", axis=1)\n\n       CODE_IRIS  DP_score_1_mars_neigh_1  ...  DP_score_4_in_20  DP_score_5_in_20\n0      751186920                 0.208233  ...          0.172082          0.165246\n1      751166219                 0.162854  ...          0.213208          0.267179\n2      751166305                 0.163832  ...          0.222252          0.252626\n3      751197403                 0.243304  ...          0.169853          0.164422\n4      751197315                 0.262427  ...          0.168257          0.163542\n...          ...                      ...  ...               ...               ...\n11200  751155803                 0.169643  ...          0.232087          0.185308\n11201  751155815                 0.166650  ...          0.255433          0.191505\n11202  751155815                 0.170142  ...          0.227534          0.183985\n11203  751155815                 0.169949  ...          0.229324          0.184510\n11204  751155816                 0.165405  ...          0.263685          0.193772\n\n[11205 rows x 101 columns]\n\n\nLet us look at the number of unique IRIS.\n\nlen(np.unique(debiased_groups_scores.reset_index().id))\n\n11151\n\n\nThe number of values is lower the number of rows in the table with debiased scores. Let us isolate the duplicated rows:\n\nduplicate_rows = debiased_groups_scores[\n  debiased_groups_scores.reset_index().duplicated(subset='id', keep=False)\n  ]\n\nWe can look at those:\n\nduplicate_rows.drop(\"id\", axis=1)\n\n      CODE_IRIS  DP_score_1_mars_neigh_1  ...  DP_score_4_in_20  DP_score_5_in_20\n193   751186919                 0.174791  ...          0.191386          0.172225\n194   751186919                 0.174792  ...          0.191387          0.172224\n195   751186919                 0.174791  ...          0.191387          0.172224\n196   751186919                 0.174791  ...          0.191386          0.172224\n197   751186919                 0.174791  ...          0.191387          0.172224\n...         ...                      ...  ...               ...               ...\n2536  751093603                 0.167135  ...          0.251988          0.190650\n4101  751020703                 0.164730  ...          0.233722          0.235088\n4102  751020703                 0.164728  ...          0.233721          0.235087\n4103  751020703                 0.164554  ...          0.231059          0.239069\n4104  751020703                 0.164554  ...          0.231059          0.239070\n\n[72 rows x 101 columns]\n\n\nNow, we can save the results. Prior to that, we add the ID of the cases in a column.\n\ndebiased_groups.set_index('id', inplace=True)\ndebiased_groups_scores.set_index('id', inplace=True)\n\nThen we can save the tables:\n\ndebiased_groups.to_csv(\"../data/debiased_groups.csv\")\ndebiased_groups_scores.to_csv(\"../data/debiased_groups_scores.csv\")"
  },
  {
    "objectID": "mitigation.html#vizualization",
    "href": "mitigation.html#vizualization",
    "title": "5  Mitigation",
    "section": "5.5 Vizualization",
    "text": "5.5 Vizualization\nLet us now visualize some results. First, we can look at the observed versus predicted prices in the 12th arrondissement.\n\n\n\n\n\n\nNote\n\n\n\nThe visualization section switches to R instead of python.\n\n\n\n\nDisplay the setting codes\nlibrary(tidyverse)\nlibrary(wesanderson)\nlibrary(sf)\n# Graphs----\nfont_main = font_title = 'Times New Roman'\nextrafont::loadfonts(quiet = T)\nface_text='plain'\nface_title='plain'\nsize_title = 14\nsize_text = 11\nlegend_size = 11\n\nglobal_theme &lt;- function() {\n  theme_minimal() %+replace%\n    theme(\n      text = element_text(family = font_main, size = size_text, face = face_text),\n      legend.text = element_text(family = font_main, size = legend_size),\n      axis.text = element_text(size = size_text, face = face_text), \n      plot.title = element_text(\n        family = font_title, \n        size = size_title, \n        hjust = 0.5\n      ),\n      plot.subtitle = element_text(hjust = 0.5)\n    )\n}\n\n# Colours\ncolors_ &lt;- wes_palette('Rushmore1')\ncol_seine &lt;- \"#2140A3\"\n\n\n\n5.5.1 Data\nLet us load the real estate data (in the R session) that were cleaned in Section 1.5, in Chapter 1.\n\nload(\"../data/data_clean_all.rda\")\n\nPartiionning of the observed prices:\n\nlimits_quants &lt;- \n  data_clean_all |&gt; \n  pull(pm2) |&gt; \n  quantile(seq(0,1,0.2)) |&gt; \n  unname()\n\nLet us also load the Parisian map saved in Section 1.3 from Chapter 1.\n\nload(\"../data/shapes.rda\")\n\nIn Section 2.3 from Chapter 2, we computed the minimum distance from one iris to another, considering distances up to 30. We will also need this informations.\n\nneighbours_all &lt;- read_csv('../data/neighbours/all_neighbours_paris.csv')\n\nLet us load the mitigated predictions obtained in Section 5.4:\n\ndebiased_groups &lt;- read_csv(\"../data/debiased_groups.csv\")\ndebiased_groups_scores &lt;- read_csv(\"../data/debiased_groups_scores.csv\")\n\n\n\n5.5.2 Smoothed Prices\nLet us get the predicted classes when considering that the 12th arrondissement should be considered as protected.\n\ndebiased_groups_12 &lt;- \n  debiased_groups |&gt; \n  select(CODE_IRIS, DP_class_for_in_12, id) |&gt; \n  unique()\n\nWe discretize into quantile-based classes the selling and estimated price, and then calculate the average in each bin, at the IRIS level.\n\nsimple_regroup &lt;- \n  data_clean_all  |&gt; \n  # Discretize\n  mutate(\n    cut_observed = cut(\n      pm2, \n      limits_quants, \n      c(1:(length(limits_quants) - 1))\n    )\n  ) |&gt;  \n  mutate(\n    cut_predictions = cut(\n      pm2_estimated, \n      limits_quants, \n      c(1:(length(limits_quants) - 1))\n    )\n  ) |&gt; \n  # Average in each bin\n  group_by(cut_observed) |&gt; \n  mutate(\n    mean_observed_cut = mean(pm2)\n  ) |&gt; \n  ungroup() |&gt; \n  group_by(cut_predictions) |&gt; \n  mutate(\n    mean_estimated_cut = mean(pm2_estimated)\n  ) |&gt; \n  ungroup() |&gt; \n  select(id, CODE_IRIS, pm2, contains('cut')) |&gt; \n  unique() %&gt;%\n  {.-&gt;&gt; data_mean_per_group} |&gt; \n  group_by(CODE_IRIS) |&gt; \n  summarise(\n    mean_real = mean(pm2), \n    mean_cut = mean(mean_estimated_cut)\n  )\nsimple_regroup\n\n# A tibble: 826 × 3\n   CODE_IRIS mean_real mean_cut\n   &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt;\n 1 751010101    11288.   13685.\n 2 751010201    11798.   12391.\n 3 751010202    12887.   12699.\n 4 751010203    13550.   12792.\n 5 751010204    13631.   12204.\n 6 751010301    12743.   13054.\n 7 751010401    14040.   12978.\n 8 751020601    13230.   12699.\n 9 751020701    12864.   12463.\n10 751020702    12267.   11789.\n# ℹ 816 more rows\n\n\nIn the process, we also created a tibble (data_mean_per_group) with all the observations from the real-estate data, giving the ID of the observation, the IRIS it is from, the observed price, the bin the observed price is in, the bin the estimated price is in, augmented with the average price (selling and observed) in the corresponding bins\nFor example, for the first row in the data:\n\ndata_mean_per_group |&gt; slice(1) |&gt; select(-id) |&gt; as.data.frame()\n\n  CODE_IRIS  pm2 cut_observed cut_predictions mean_observed_cut\n1 751186920 9500            2               2          9431.742\n  mean_estimated_cut\n1           9445.654\n\ndata_clean_all$pm2_estimated[1]\n\n[1] 9227.013\n\n\nLet us extract the average estimated price in each constituted bins, in the whole dataset.\n\nmapping_prices &lt;- \n  data_mean_per_group |&gt; \n  select(id, CODE_IRIS, cut_predictions, mean_estimated_cut) |&gt; \n  select(cut_predictions, mean_estimated_cut) |&gt; \n  unique() |&gt; \n  select(\n    group_cut = cut_predictions, \n    value_cut = mean_estimated_cut\n  )\n\nFor each observation in the dataset, we have (in data_mean_per_group):\n\nthe IRIS (CODE_IRIS)\nthe selling price (pm2)\nthe quintile-bin the selling price is in (cut_observed)\nthe average selling price in that bin (mean_observed_cut)\nthe quintile-bin the estimated price is in (cut_predictions)\nthe average selling price in that bin (mean_estimated_cut).\n\nIn the tibble debiased_groups_12, we have, for the same observations:\n\nthe IRIS (CODE_IRIS)\nthe fair predicted class (DP_class_for_in_12).\n\nIn the tibble mapping_prices, we have:\n\nthe average estimated price in each quintile-defined bins (value_cut)\nthe bins they refer to (group_cut).\n\nFor each observation, we will consider the average selling price in the bin defined used the estimated price (mean_estimated_cut). At the IRIS level, we calculate the average of those values to obtain mean_cut_orig. This corresponds to the average estimated price.\nWe also consider the average estimated price in each quintile-defined bins (value_cut) the observations have be assigned to after mitigation, and compute, at the IRIS level, the average of those values to obtain mean_cut_mitig. This corresponds to the average price after mitigation.\nThen, we compute the difference between the two: mean_cut_mitig - mean_cut_orig.\n\ndiff_viz &lt;- \n  data_mean_per_group |&gt; \n  select(id, CODE_IRIS, cut_predictions, mean_estimated_cut) |&gt; \n  right_join(\n    debiased_groups_12 |&gt; \n      select(id, DP_class_for_in_12),\n    relationship = \"many-to-many\"\n  ) |&gt; \n  mutate(DP_class_for_in_12 = as.factor(DP_class_for_in_12)) |&gt; \n  left_join(\n    mapping_prices, \n    by = c('DP_class_for_in_12'='group_cut')\n  ) |&gt; \n  group_by(CODE_IRIS) |&gt; \n  summarise(\n    mean_cut_orig = mean(mean_estimated_cut), \n    mean_cut_mitig = mean(value_cut)\n  ) |&gt; \n  mutate(diff = mean_cut_mitig - mean_cut_orig)\n\nJoining with `by = join_by(id)`\n\n\nNow that we have obtained differences between predicted prices before and after mitigation, let us spatially smooth these values (see Chapter 2). We consider here the neighbors distant up to 8.\n\nsmoothed_diff &lt;- \n  neighbours_all |&gt; \n  mutate(distance = distance + 1) |&gt; \n  filter(distance &lt;= 7) |&gt; \n  mutate(\n    distance = if_else(from_iris == to_iris, 1, distance),\n    to_iris = as.character(to_iris)\n  ) |&gt; \n  left_join(\n    diff_viz,\n    by = c('to_iris'='CODE_IRIS')\n  ) |&gt; \n  mutate(inverse_weight = 1/distance) |&gt; \n  mutate(value = diff * inverse_weight) |&gt; \n  drop_na() |&gt; \n  group_by(from_iris) |&gt; \n  summarise(\n    total_weights = sum(inverse_weight), \n    total_value = sum(value)\n  ) |&gt; \n  mutate(smoothed_diff = total_value / total_weights) |&gt; \n  ungroup() |&gt; \n  mutate(CODE_IRIS = as.character(from_iris), smoothed_diff)\n\n\n\n5.5.3 Plots\nNow we can visualize the impact of the mitigation. First, we create a graph (Figure 5.2) that shows the under- and overvaluation in the real-estate market in the 12th arrondissement.\n\n\nDisplay the (R) codes used to create the Figure.\nbins &lt;- 10\n\ncalib_over &lt;- \n  data_clean_all |&gt; \n  filter(NOM_COM == 'Paris 12e Arrondissement') |&gt; \n  filter(\n    pm2 &lt; 14000 & pm2 &gt; 5000,\n    pm2_estimated &lt; 14000 & pm2_estimated &gt; 5000\n  ) |&gt; \n  mutate(is_under = if_else(pm2 &gt; pm2_estimated, 'undervalued', 'overvalued')) %&gt;%\n  {.-&gt;&gt; data_points_12} |&gt; \n  ggplot() + \n  geom_point(\n    mapping = aes(\n      x = pm2_estimated, \n      y = pm2, \n      color = is_under)\n  ) + \n  geom_point(\n    mapping = aes(\n      x = estim_quant, \n      y = observed_quant\n    ), \n    data = tibble(\n      estim_quant = quantile(\n        data_points_12$pm2_estimated, \n        seq(0.1,0.9,1/bins)),\n      observed_quant = quantile(\n        data_points_12$pm2, \n        seq(0.1,0.9,1/bins)),\n    )\n  ) + \n  geom_abline(\n    slope=1, \n    color='lightgrey', \n    lty='dashed', \n    lwd=1\n  ) + \n  xlim(c(5000, 14000)) + \n  ylim(c(5000, 14000)) + \n  scale_color_manual(values=c(colors_[2], colors_[3])) + \n  global_theme() + \n  guides(color = guide_legend(title='', override.aes = list(size=5))) + \n  xlab(expression(paste('Price ', m^2, ' estimated'))) + \n  ylab(expression(paste('Price ', m^2, ' observed'))) +\n  ggtitle(\"Raw Predictions\") +\n  theme(legend.position = \"bottom\")\n\ncalib_over\n\n\n\n\nFigure 5.2: Predictions in the 12th arrondissement.\n\n\n\n\n\nNow, let us visualize on a map (Figure 5.3) how the estimated values change after the correction of the unfairness bias, in the 12th arrondissement.\n\n\nDisplay the (R) codes used to create the Figure.\np_mitig &lt;- \n  shapes_paris |&gt; \n  left_join(smoothed_diff, by = \"CODE_IRIS\") |&gt; \n  ggplot() + \n  geom_sf(mapping = aes(fill = smoothed_diff)) + \n  geom_sf(\n    data=shapes_seine,\n    fill = col_seine\n  ) +\n  geom_sf(\n    data = shapes_paris |&gt; \n      filter(NOM_COM == \"Paris 12e Arrondissement\") |&gt; \n      st_union(),\n    colour = \"white\", lty = 2, linewidth = 1,\n    fill = NA\n  ) +\n  global_theme() + \n  guides(fill = guide_legend(title = 'Adjustment')) +\n  theme(legend.position = 'bottom') + \n  ggtitle(expression(paste(\n    'Mitigated Predictions in ', 12^italic('th'), italic('Arrondissement')))\n  )\np_mitig\n\n\n\n\nFigure 5.3: Predictions in the 12th arrondissement.\n\n\n\n\n\n\n\n\n\nDenis, Christophe, Romuald Elie, Mohamed Hebiri, and François Hu. 2021. “Fairness Guarantee in Multi-Class Classification.” arXiv Preprint arXiv:2109.13642."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Alghamdi, Wael, Hsiang Hsu, Haewon Jeong, Hao Wang, Peter Michalak,\nShahab Asoodeh, and Flavio Calmon. 2022. “Beyond Adult and COMPAS:\nFair Multi-Class Prediction via Information Projection.”\nAdvances in Neural Information Processing Systems 35: 38747–60.\n\n\nCalders, T., F. Kamiran, and M. Pechenizkiy. 2009. “Building\nClassifiers with Independency Constraints.” In IEEE\nInternational Conference on Data Mining.\n\n\nDenis, Christophe, Romuald Elie, Mohamed Hebiri, and François Hu. 2021.\n“Fairness Guarantee in Multi-Class Classification.”\narXiv Preprint arXiv:2109.13642.\n\n\nHardt, M., E. Price, and N. Srebro. 2016. “Equality of Opportunity\nin Supervised Learning.” In Neural Information Processing\nSystems.\n\n\nKrüger, Fabian, and Johanna F. Ziegel. 2020. “Generic Conditions\nfor Forecast Dominance.” Journal of Business & Economic\nStatistics 39 (2021): 972–83.\n\n\nNaeini, Mahdi Pakdaman, Gregory Cooper, and Milos Hauskrecht. 2015.\n“Obtaining Well Calibrated Probabilities Using Bayesian\nBinning.” In Proceedings of the AAAI Conference on Artificial\nIntelligence. Vol. 29. 1.\n\n\nWidmann, David, Fredrik Lindsten, and Dave Zachariah. 2019.\n“Calibration Tests in Multi-Class Classification: A Unifying\nFramework.” Advances in Neural Information Processing\nSystems 32."
  }
]